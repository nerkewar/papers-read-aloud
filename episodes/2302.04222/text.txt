GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models.

Abstract.

Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after “fine-tuning” on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply “style cloaks” to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05),
Glaze is highly successful at disrupting mimicry under normal conditions (&gt;92%) and against adaptive countermeasures (&gt;85%).

1. Introduction.

It is not an exaggeration to say that the arrival of text-to-image generator models has transformed, perhaps upended, the art industry. By sending simple text prompts like “A picture of a corgi on the moon” to diffusion models such as StableDiffusion or MidJourney, anyone can generate incredibly detailed, high resolution artwork that previously required many hours of work by professional artists. AI-art such as those in Figure 1 have won awards at established art conventions , served as cover images for magazines , and used to illustrate children’s books  and video games . More powerful models continue to arrive , catalyzed by VC funding , technical research breakthroughs , and powered at their core by continuous training on a large volume of man-made art scraped from online art repositories such as ArtStation, Pinterest and DeviantArt.

Only months after their arrival, these models are rapidly growing in users and platforms. In September 2022, MidJourney reported over 2.7 million users and 275K AI art images generated each day. Beyond simple prompts, many have taken the open sourced StableDiffusion model, and “fine-tuned” it on additional samples from specific artists, allowing them to generate AI art that mimics the specific artistic styles of that artist . In fact, entire platforms have sprung up where home users are posting and sharing their own customized diffusion models that specialize on mimicking specific artists, likeness of celebrities, and NSFW themes .

Beyond open legal questions of copyrights, intellectual property, and consent, it is clear that these AI models have had significant negative impacts on independent artists. For the estimated hundreds of thousands of independent artists across the globe, most work on commissions, and attract customers by advertising and promoting samples of their artwork online. First, professional artists undergo years of training to develop their individual artistic styles. A model that mimics this style profits from that training without compensating the artist, effectively ending their ability to earn a living. Second, as synthetic art mimicry continues to grow for popular artists, they displace original art in search results, further disrupting the artist’s ability to advertise and promote work to potential customers . Finally, these mimicry attacks are demoralizing art students training to be future artists. Art students see their future careers replaced by AI models even if they can successfully find and develop their own artistic styles.

Today, all of these consequences have indeed occurred in the span of a few months. Art students are quitting the field; AI models that mimic specific artists are uploaded and shared for free; and professional artists are losing their livelihoods to models mimicking their unique styles. Artists are fighting back via lawsuits , online boycotts and petitions , but legal and regulatory action can take years, and are difficult to enforce internationally. Thus most artists are faced a choice to 1) do nothing, or 2) stop sharing samples of their art online to avoid training models, and in doing so cripple their main way to advertise and promote their work to customers.

In this paper, we present the design, implementation and evaluation of a technical alternative to protect artists against style mimicry by text-to-image diffusion models. We present Glaze, a system that allows an artist to apply carefully computed perturbations to their art, such that diffusion models will learn significantly altered versions of their style, and be ineffective in future attempts at style mimicry. We worked closely with members of the professional artist community to develop Glaze, and conduct multiple user studies with 1,156 participants from the artist community to evaluate its efficacy, usability, and robustness against a variety of active countermeasures.

Intuitively, Glaze works by taking a piece of artwork, and computing a minimal perturbation (a “style cloak”) which, when applied, shifts the artwork’s representation in the generator model’s feature space towards a chosen target art style. Training on multiple cloaked images teaches the generator model to shift the artistic style it associates with the artist, leading to mimicry art that fails to match the artist’s true style.

Our work makes several key contributions:

1. We engage with top professional artists and the broader community, and conduct user studies to understand their views and concerns towards AI art and the impact on their careers and community.
2. We propose Glaze, a system that protects artists from style mimicry by adding minimal perturbations to their artwork to mislead AI models to generate art different from the targeted artist. 92% of surveyed artists find the perturbations small enough not to disrupt the value of their art.
3. Surveyed artists find that Glaze successfully disrupts style mimicry by AI models on protected artwork. 93% of artists rate the protection is successful under a variety of settings, including tests against real-world mimicry platforms.
4. In challenging scenarios where an artist has already posted significant artworks online, we show Glaze protection remains high. 87.2% of surveyed artists rate the protection as successful when an artist is only able to cloak 1/4 of their online art (75% of art is uncloaked).
5. We evaluate Glaze and show that it is robust (protection success &gt; 85%) to a variety of adaptive countermeasures.

Ethics.
Our user study was reviewed and approved by our institutional review board (IRB). All art samples used in experiments were used with explicit consent by their respective artists. All user study participants were compensated for their time, although many refused payment.

2. Background: AI Art and Style Mimicry.

In this section, we provide critical context in the form of basic background on current AI art models and style mimicry.

2.1 Text-to-Image Generation.

Since Text-to-image generation was first proposed in 2015 , a stream of research has proposed newer model architectures and training methods enabling generation of higher-quality images . The high level design of recent models used for AI art generation  is shown in Figure 3. During training, the model takes in an image.
x and uses a feature extractor Phi to extract its features, producing.
Phi (x). Simultaneously, a conditional image generator G takes in a corresponding text caption (s) and outputs a predicted feature vector.
G(s). Then the parameters of G are optimized so the text feature vector.
G(s) matches the image feature vector Phi (x). At generation time, a user gives G a text prompt s0, and G outputs an image feature vector.
G(s0). A decoder D then decodes G(s0) to produce the final generated image.

Compared to earlier models based on generative adversarial networks (GANs) or variational autoencoders (VAE) , more recent models leveraging diffusion models produce significantly higher quality images. Feature extractor (Phi ) is used to reduce the dimensionality of the input image to facilitate the generation process. The extractor Phi and decoder D are often a pair of variational autoencoder (VAE) , that is,extractor (encoder) extracts image features and decoder map features back to images.

Training Data Sources. 
The training datasets of these models typically contain image/ALT text pairs scraped from the Internet. They are extremely large, for example LAION  contains 5 billion images collected from 3 billion webpages.

These datasets are subject to minimal curation and governance. Data collectors typically only filter out data with extremely short or incorrect text captions (based on an automated text/image alignment metric ). Since copyrighted images are not filtered , these datasets are rife with private, sensitive content, including copyrighted artworks.

2.2 Style Mimicry.

In a style mimicry attack, a bad actor uses an AI art model to create art in a particular artist’s style without their consent. More than 67% of art pieces showcased on a popular AI-art-sharing website leverage style mimicry .

Style mimicry techniques.
Today, a “mimic” can easily copy the style of a victim artist with only an open-source text-to-image model and a few samples of artwork from the artist. A naive mimicry attack directly queries a generic text-to-image model using the name of the victim artist. For example, the prompt “a painting in the style of Greg Rutkowski” would cause the model to generate images in the style of Polish artist Greg Rutkowski. This is because many of Rutkowski’s artworks appear in training datasets of these generic models labeled with his name.

Naive mimicry can succeed when the artist is well-known and has a significant amount of art online, but fail on other artists. In more recent mimicry attacks, a mimic fine-tunes a generic text-to-image model on samples of a target artist’s work (as few as 20 unique pieces) downloaded from online sources. This calibrates the model to the victim artist’s style, identifying important features related to the victim style and associating these regions in the feature space with the victim artist’s name . This enables style mimicry with impressive accuracy. The entire fine-tuning process takes less than 20 minutes on a low-end consumer GPU ([1] It takes an average of 18.3 minutes on a GTX 1080 GPU).

Real-work mimicry incidents. 

The first well-known incident of mimicry was when a Reddit user stole American artist Hollie Mengert’s style and open-sourced the style-specific model on Reddit . Figure 2 has a side-by-side comparison of Hollie’s original artwork and plagiarized artwork generated via style mimicry. Later, famous cartoonist Sarah Andersen reported that AI art models can mimic her cartoon drawings , and other similar incidents abound .

Several companies  have even hosted style mimicry as a service, allowing users to upload a few art pieces painted by victim artists and producing new art in the victim styles. CivitAI built a large online marketplace where people share their customized stable diffusion models, fine-tuned on certain artwork.

3. Collaborating with Artists.

Next, we explain our collaborative relationship with professional artists, and its significant impact on our key evaluation metrics in this paper. We also summarize key results from our first user study on views of AI art and mimicry by members of the artist community.

Artists have spoken out against style mimicry in numerous venues, focusing particularly on how it violates their intellectual property rights and threatens their livelihoods . Others have taken direct action. The Concept Art Association raised over $200K to fight AI art , and filed a class action lawsuit in the US against AI art companies . In November 2022, artists organized a large protest against ArtStation , the large digital art sharing platform that allowed users to post AI artwork without identification. Anti-AI images flooded the site for several weeks, until ArtStation banned the protest images .

Members of the professional art community reached out to us in Sept 2022. We joined online town halls and meetings alongside hundreds of professionals, including Emmy winners and artists at major film studios. After learning more, we began an active collaboration with multiple professional artists, including award-winning artist Karla Ortiz, who leads efforts defending artists and is lead plaintiff in the class action suit. The artists helped this project in multiple ways, by 1) sharing experiences about specific ways AI-art has impacted them and their colleagues; 2) sharing domain knowledge about what is acceptable to artists in terms of perturbations on their art; and 3) helping to widely disseminate our user study to members of their professional organizations, including the Concept Art Association and the Animation Guild (TAG839).

Evaluation via Direct Feedback from Artists.
Our goal is help artists disrupt AI models trying to mimic their artistic style, without adversely impacting their own artwork. Because “success” in this context is highly subjective (“Did this AI-art successfully mimic Karla’s painting style?”), we believe the only reliable evaluation metric is direct feedback by professional artists themselves. Therefore, wherever possible, the evaluation of Glaze is done via detailed user studies engaging members of the professional artist community, augmented by an empirical score we develop based on genre prediction using CLIP models.

We deployed two user studies during the course of this project (see Table 1). Both are IRB-approved by our institution. Both draw participants from professional artists informed via their social circles and professional networks. The first (Survey 1, §3.1, §6.3), asked participants about their broad views of AI style mimicry, and then presented them with a number of inputs and outputs of our tool, and asked them to give ratings corresponding to key metrics we wanted to evaluate. We select a subset of participants from the first study to participate in a longer and more in-depth study (Survey 2) where they were asked to evaluate the performance of Glaze in additional settings (§6.3, §6.4, §7, and Appendix A).

3.1 Artists’ Opinions on Style Mimicry.

While we expected artists to view style mimicry negatively, we wanted to better understand how much individual artists understood this topic and how many perceived it as a threat. Here we describe results from Survey 1 to gather perceptions of the potential impact of AI art on existing artists.

Survey Design.
Our survey consisted of both multiple choice and free response questions to understand how well people understand the concept of AI art, and how well the models successfully imitate the style of artists. Additionally, we asked artists about the extent to which they anticipate the emergence of AI art to impact their artistic activities, such as posting their art online and their job security. A handful of professional artists helped disseminate our survey to their respective artist community groups. Overall, we collected responses from 1,207 participants, consisting primarily of professional artists (both full-time (46%) and part-time/freelancer (50%)) and some non-artist members of the art community who felt invested in the impact of AI art (4%). Of the participants who consider themselves artists, their experience varied: &lt;1 year (13%), 1-5 years (49%), 5-10 years (19%), 10+ years (19%). Participants’ primary art style varied widely, including: animation, concept art, abstract, anime, game art, digital 2D/3D, illustration, character artwork, storyboarding, traditional painting/drawing, graphic design, and others.

Key Results.
Our study found that 91% of the artists have read about AI art extensively, and either know of or worry about their art being used to train the models. Artists expect AI mimicry to have a significant impact on artist community: 97% artists state it will decrease some artists’ job security;
88% artists state it will discourage new students from studying art; and.
70% artists state it will diminish creativity. “Junior positions will become extinct,” as stated by one participant.

Many artists (&gt; 89% artists) have already or plan to take actions because of AI mimicry. Over 95% of artists post their artwork online. Out of these artists, 53% of them anticipate reducing or removing their online artwork, if they haven’t already. Out of these artists, 55% of them believe reducing their online presence will significantly impact their careers. One participant stated “AI art has unmotivated myself from uploading more art and made me think about all the years I spent learning art.” 78% of artists anticipate AI mimicry would impact their job security, and this percentage increases to 94% for the job security of newer artists. Further, 24% of artists believe AI art has already.
impacted their job security, and an additional 53% expect to be affected within the next 3 years. Over 51% of artists expressed interest in proactive measures, such as personally joining class action lawsuits against AI companies.

Professional artists thought AI mimicry was very successful at mimicking the style of specific artists. We showed the artists examples of original artwork from 23 artists, and the artwork generated by a model attempting to mimic their styles (detailed mimicry setup in §6). 77% of artists found the AI model.
successfully or very successfully mimic the styles of victim artists, with one stating “it’s shocking how well AI can mimic the original artwork.” Additionally, 19% of participants thought the AI mimicry is somewhat successful, leaving only &lt;5% of artists rating the mimicry as unsuccessful. Several artists also pointed out that, as artists, upon close inspection they could spot differences between the AI art and originals, but were skeptical the general public would notice them.

A significant concern of most participants, surprisingly, is not just the existence of AI art, but rather scraping of existing artworks without permission or compensation. As one participant stated: “If artists are paid to have their pieces be used and asked permission, and if people had to pay to use that AI software with those pieces in it, I would have no problem.” However, without consent to use their artwork to train the models, “it’s incredibly disrespectful to the artist to have their work ‘eaten’ by a machine [after] many years to grow our skills and develop our styles.”

4. Preliminaries.

We propose Glaze, a tool that protects artists against AI style mimicry. An artist uses Glaze to add small digital perturbations (“cloak”) to images of their own art before sharing online (Figure 5). A text-to-image model that trains on cloaked images of artwork will learn an incorrect representation of the artist’s style in feature space that is,the model’s internal understanding of artistic styles. When asked to generate art pieces in victim’s style, the model will fail to mimic the style of the victim, and instead output art pieces in a recognizably different style.

Here, we first introduce the threat model, then discuss existing alternatives to the AI style mimicry problem. We present the intuition behind Glaze and detailed design in §5. 4.1 Threat Model.

Here we state assumptions for both the artists protecting their own art and the users training models to replicate their artistic style. We refer to these AI art model trainers as “mimics.”

Artists.
Artists want to share and promote their artwork online without allowing mimics to train models that replicate their art styles. Sharing art online enables artists to sell their work and attract commissioned work, fueling their livelihoods (§3). Artists protect themselves by adding imperceptible perturbations to their artwork before sharing them as shown in Figure 5. The goal of the.
Glaze cloak is to disrupt the style mimicry process, while only introducing minimal perturbation on images of the artwork.

We assume the artists have access to moderate computing resources (for example,a laptop) and add perturbation to images of their artwork locally before posting online. We also assume artists have access to some public feature extractor (for example,open-source models such as Stable Diffusion). We begin with assumption that artists use the same feature extractor as mimics (large majority of mimics use the open-source Stable Diffusion model). We later relax this assumption.

Mimics. 
The mimic’s goal is to train a text-to-image model that generates high-quality art pieces of any subject in the victim’s style. A mimic could be a well-funded AI company, for example,Stability AI or OpenAI, or an individual interested in the style of victim artist. We assume the mimic has:

1. access to the weights of generic text-to-image models well-trained on large datasets;
2. access to art pieces from the target artist;
3. significant computational power.

We assume the attack scenario where the mimic fine-tunes its model on images of the artist’s artwork (as shown in Figure 4). This is stronger than the naive mimic attack without fine tuning. Finally, we assume the mimic is aware of our protection tool and can deploy adaptive countermeasures (§7).

4.2 Potential Alternatives and Challenges.

A number of related prior works target protection against invasive and unauthorized facial recognition models. They proposed “image cloaking” as a tool to prevent a user’s images from being used to train a facial recognition model of them . They share a similar high level approach, by using optimized perturbations that cause cloaked images to have drastically different feature representations from original user images. It is possible to adapt existing cloaking-based systems to protect artists against AI style mimicry. Protection system would compute a cloak on each artwork in order to perturb its feature space representation to be different from its unperturbed representation. This can succeed if the cloak significantly shifts the artwork’s feature representation, making resulting models generate dramatically different artwork.

We found that in practice, however, existing solutions are unable to introduce large-enough feature space shifts to achieve the desired protection. This is due to the properties of feature spaces in text-to-image models. Face recognition models classify identities, so their feature spaces mainly represent identity-related information. On the other hand, text-to-image models reconstruct original images from extracted features, so their feature spaces retain more information about the original image (objects, locations, color, style, and so on). Thus, producing the same shift in feature representation in a text-to-image model is much harder (requires more perturbation budget) than in a classification model. This observation is validated by prior work showing that adversarial perturbations are much less effective at attacking generative models . Specifically, found that adversarial attack methods that are effective at attacking classifiers are significantly less effective at attacking autoencoders. We empirically confirm that existing cloaking methods cannot prevent AI mimicry (§A.1 in Appendix). We show that Fawkes  and LowKey  perform poorly in this setting, even when artists add highly visible cloaks to their artwork.

For generative models, concurrent work proposes PhotoGuard, a method to cloak images to prevent unauthorized image edits (inpainting) on cloaked images. Similar to existing cloaking systems, PhotoGuard tries to indiscriminately minimize all information contained in an image (that is,the norm of the feature vector) to prevent models from editing the image. Thus, it is also not effective at mimicry prevention (validated by empirical results in Appendix A.1).

Design Challenges. 
The main reason that existing cloaking methods fail to prevent AI mimicry is because they indiscriminately shift all features in an image, wasting the cloak perturbation budget on shifting unnecessary features (for example,object shape, location, and so on). Protecting artist’s style requires only shifting features related to the artistic style of victim. This can be achieved if a text-to-image model learns to draw objects similar to those drawn by the victim artist as long as the model cannot mimic the artist’s unique style. Thus, optimal protection from mimicry requires concentrating the cloak on style-specific features.

Unfortunately, identifying and separating out these style-specific features is difficult. Even assuming the existence of interpretability methods that perfectly explain the feature space of a text-to-image model, there is no clear way to mathematically define and calculate “artistic styles.” In all likelihood, any definition would change across different styles. For example, “impressionist” likely correlates more strongly with color features, whereas “cubism” correlates with shape features. Even across multiple art pieces in the same style, the style may manifest differently.

5. Disrupting Style Mimicry with Glaze.

In this section, we introduce Glaze, its design intuition followed by the detailed algorithm.

5.1 Design Intuition.

Our key intuition is to identify and isolate style-specific features.
of an artist’s original artwork, that is,the set of image features that correspond to artistic style. Then Glaze computes cloaks while focusing the perturbation budget on these style-specific features to maximize impact on stylistic features.

As discussed, identifying and calculating style-specific features in model’s feature space is difficult due to the poor interpretability of model features and how art style manifests differently across artworks. We overcome these two challenges by designing a style-dependent and artwork-dependent method that operates at image space. Given an artwork, we leverage “style transfer,” an end-to-end computer vision technique, to modify and isolate its style components. “Style transfer” transforms an image into a new image with a different style (for example,from impressionist style to cubist style) while keeping other aspects of the image similar (for example,subject matter and location).

We leverage style transfer in our protection technique as follows. Given an original artwork from the victim artist, we apply style-transfer to produce a similar piece of art with a different style, for example,in style of “an oil painting by Van Gogh” (Figure 6 a). The new version has similar content to the original, but its style mirrors that of Van Gogh. We show more style-transfer examples with different target styles in Figure 7. Now, we can use the style-transferred artwork as projection target to guide the perturbation computation. This perturbs the original artwork’s style-specific features towards that of the style-transferred version. We do this by optimizing a cloak that, when added to the original artwork, makes its feature representation similar to the style-transferred image. Since the content is identical between the pair of images, cloak optimization will focus its perturbation budget on style features.

5.2 Computing Style Cloaks.

Using this approach, we compute style cloaks to disrupt style mimicry as follows. Given an artwork (x), we use an existing feature extractor to compute the style-transferred version of x into target style T: Omega (x,T). We then compute a style cloak delta x, such that delta x moves x’s style-specific feature representation to match that of Omega (x,T) while minimizing visual impact. The cloak generation optimization is:

where Phi is a generic image feature extractor commonly used in text-to-image generation tasks, Dist(.) computes the distance of two feature representations, |delta x| measures the perceptual perturbation caused by cloaking, and p is the perceptual perturbation budget.

As discussed in §5.1, the use of the style-transferred image Omega (x,T) guides the cloak optimization in Eq (1) to focus on changing style-specific image features. To maximize cloak efficacy, the target style T should be dissimilar from artist’s original style in the feature space. We discuss our heuristic for selecting target styles in §5. 5.3 Detailed System Design.

Now we present the detailed design of Glaze. Given a victim artist V, Glaze takes as input the set of V’s artwork to be shared online XV, an image feature extractor Phi , an style-transfer model.
Omega , and perturbation budget p. Note that in many cases, a single model (for example Stable Diffusion) provides both Phi and Omega.

Step 1: Choose Target Style.
The selected target style T.
should be sufficiently different from V’s style in model feature space to maximize chances of disrupting style mimicry. For example, Fauvism and Impressionism are distinct art styles that often look visually similar to the untrained eye. Image of an impressionist painting style cloaked to Fauvism might not produce a visually discernible effect on model-generated paintings. Note that an artist can maximize their ability to avoid mimicry if they consistently style cloak all their artwork towards the same target T.

For a new user, Glaze uses the following algorithm to randomly select T from a set of candidate styles reasonably different from V’s style. The algorithm first inspects a public dataset of artists, each with a specific style (for example,Monet, Van Gogh, Picasso). For each candidate target artist/style, it selects a few images in that style and calculates their feature space centroid using.
Phi. It also computes V’s centroid in Phi using V’s artwork. Then, it locates the set of candidate styles whose centroid distance to V’s centroid is between the 50 to 75 percentile of all candidates. Finally, it randomly selects T from the candidate set.

Step 2: Style transfer. 
Glaze then leverages a pre-trained style-transfer model Omega to generate the style-transferred artwork for optimization. Given each art piece x in XV and target style T, it style transfers x to target style T to produce style-transferred image Omega (x,T).

Step 3: Compute cloak perturbation.
Then, Glaze computes the cloak perturbation, delta x for x, following the optimization defined by eq. (1), subject to |delta x|&lt;p. Our implementation uses LPIPS (Learned Perceptual Image Patch Similarity)  to bound the perturbation. Different from the Lp distance used in previous work , LPIPS has gained popularity as a measure of user-perceived image distortion . Bounding cloak generation with this metric ensures that cloaked versions of images are visually similar to the originals. We apply the penalty methodto solve the optimization in eq.(1) as follows:

where alpha controls the impact of the input perturbation. L2.
distance is used to calculate feature space distance.

Upload artwork online.
Finally, the artist posts the cloaked artwork online. For artists already with a large online presence, they can cloak and re-upload artwork on their online portfolio. While updating online images is not always possible,
Glaze can be effective even when the mimic’s model has significant amount of uncloaked art (§6.4).

5.4 On the Efficacy of Style Cloaks.

Glaze’s style cloaks work by shifting feature representation of artwork in the generator model. But how much shift do we need in order to have a noticeable impact on mimicked art?

Two reasons suggest that even small shifts in style will have a meaningful impact in disrupting style mimicry. First, generative models used for style mimicry have continuous output spaces, that is,any shift in image feature representation results in changes in the generated image. Because generative models are trained to interpolate their continuous feature spaces , any shift in the model’s representation of art style results in a new style, a “blend” between the artist and the chosen target style. Second, mimicked artwork must achieve reasonable quality and similarity in style to the artist to be useful. Small shifts in the style space often produce incoherent blends of conflicting styles that are enough to disrupt style mimicry, for example,thick oil brushstrokes of Van Gogh’s style mixed into a realism portrait.

These two factors contribute to Glaze’s success in more challenging scenarios (§6.4), and its robustness against countermeasures (for example adversarial training) that succeed against cloaking tools for facial recognition (§7).

6. Evaluation.

In this section, we evaluate Glaze’s efficacy in protecting artists from style mimicry. We first describe the datasets, models, and experimental configurations used in our tests. Then we present the results of Glaze’s protection in a variety of settings. Due to Glaze’s highly visual nature, we evaluate its performance using both direct visual assessment by.
human artists in a user study, and automated metrics (see §6.2 for details).

Summary of results.
Over 93% of artists surveyed believe Glaze.
effectively protects artists’ styles from AI style mimicry attacks. Protection efficacy remains high in challenging settings, like when the mimic has access to unprotected artwork. Glaze also achieves high protection performance against a real-world mimicry-as-a-service platform. Of our 1156 artist participants, over 92% found the perturbations introduced by cloaking small enough not to disrupt the value of their art, and over 88% would like to use Glaze to protect their own artwork from mimicry attacks.

6.1 Experiment Setup.

Mimicry dataset. 
We evaluate Glaze’s performance in protecting the styles of the following two groups of artists:

1. Current artists: 4 professional artists let us use their artwork in our experiments. These artists have different styles and backgrounds (for example, full-time/freelancers, watercolor painters/digital artists, well-known/independent). Each provided us with between 26 to 34 private original art pieces for our experiments. We use perceptual hashing  to verify that none of these are included in existing public datasets used to train generic text-to-image models (for example ).
2. Historical artists: We also evaluate Glaze’s protection on 195 historical artists (for example, van Gogh, Monet) from the WikiArt dataset . The WikiArt dataset contains 42,129 art pieces from 195 artists. Each art piece is labeled with its genre (for example, impressionism, cubism). We randomly sampled 30 art pieces from each artist to use in style mimicry attacks. Generic text-to-image models found online have been trained on some artwork from these artists. Using this art simulates a more challenging scenario in which a famous artist attempts to disrupt a model that already understands their style.

Mimicry attack setup. 
We recreate the strongest-possible mimicry attack scenario, based on techniques used in real-world mimicry incidents , that works as follows. First, we take art pieces from the victim artist V.
and generate a text caption for each piece using an image captioning model . Then, we append the artist’s name to each caption,
for example,“mountain range by Vincent van Gogh”. Some example images and their captions are shown in Figure 16 in Appendix. Finally, we fine-tune a pre-trained generic text-to-image model (details below) on the caption/image pairs.

We use 80% of the art pieces from the victim artists to fine-tune models that mimic each artist’s style, reserving the rest for testing. We fine-tune for 3000 optimization steps, which we find achieves the best mimicry performance (Figure 17 in Appendix). We then use the fine-tuned, style-specific model to generate mimicked artwork in style of each victim artist. We query the model using the generated captions (which include V’s name) from the held-out test artwork set. We generate 5.
pieces of mimicked art for each text caption using different random seeds and compare these to the real victim art pieces with this caption. Additional details on training and generation parameters, as well as its sensitivity to random seed selection and the number of training art pieces are in Appendix A.2. Text-to-image models.
We use two state-of-the-art, public, generic text-to-image models in our experiments:

1. Stable Diffusion (SD): Stable Diffusion is a popular and high-performing open-source text-to-image model ,trained on 11.5 million images from the LAION dataset . SD training takes over 277 GPU months (on A100 GPU) and costs around $600K . SD uses diffusion methods to generate images and achieves state-of-the-art performance on several benchmarks . Viewed as one of the best open-source models, SD has powered many recent developments in text-to-image generation . We use SD version 2.1 in the paper , the most up-to-date version as of December 2022.
2. DALL⋅E-mega (DALL⋅E-m): DALL⋅E-m-mega, an updated version of the more well-known DALL⋅E-m-mini, is an open-source model based on OpenAI’s DALL⋅E-m 1 . The model leverages a VAE for image generation and is trained on 17 million images from three different datasets . Training takes 2 months on 256 TPUs . While DALL⋅E-m  performs worse than diffusion-based models like SD, we use it to evaluate how Glaze generalizes to different model architectures.

Glazeconfiguration. 
We generate cloaks for each of victim V’s art pieces following the methodology of §5.3. First, we use the target selection algorithm to select a target style T. We choose from a set of 1119 candidate target styles, collected by querying the WikiArt dataset with artist and genre names, for example,‘‘Impressionism painting by Monet’’  ([2] One artist may paint in multiple styles, resulting in multiple candidate target styles from a single artist.). We then style transfer each victim art piece into the target style leveraging the style transfer functionality of stable diffusion model (stable diffusion model has both text-to-image and style transfer functionality). We test the sensitivity of our protection results to the choice of style-transfer model in Appendix A.2. Finally, we optimize a cloak for each art piece using Equation 2 by running the Adam optimizer for 500.
steps. It takes an average of 1.2 mins on Titan RTX GPU and 7.3 mins on a single Intel i7 CPU to generate a cloak for a single piece of art.

In our initial experiments, we assume Glaze generates cloaks using the same image feature extractor as the mimic (for example SD’s or DALL⋅E-m’s feature extractor). We relax this assumption and evaluate.
Glaze’s performance when artists and mimics use different feature extractors in §6.4. 6.2 Evaluation Metrics.

We evaluate our protection performance using both visual assessment and feedback from human artists, and a scalable metric. Here, we describe the setup of our evaluation study and define the exact metrics used for evaluation.

Artist-rated protection success rate (Artist-rated PSR): 
The user studies ask artists to rate the performance of Glaze. We generate a dataset of mimicry attacks on 13 victim artists (the 4 current artists and 9.
randomly chosen historical artists) across 23 protection scenarios (including ones in §7). For each participant, we randomly select a set of mimicry attacks out of these 13×23 settings and ask them to evaluate protection success. For each mimicry attempt, we show participants 4 mimicked art pieces and 4 original art pieces from the victim artist. We ask participants to rate the success of Glaze’s protection on a 5-level Likert scale (ranging from “not successful at all” to “very successful”). Each mimicry attempt is evaluated by at least 10.
participants. We define artist-rated PSR as the percent of participants who rated Glaze’s protection as “successful” or “very successful.” Our user studies primarily focus on artists, as they would be most affected by this technology. We found though, that not all current artists despise AI art, and some view it as a new avenue for a different form of artistry.

CLIP-based genre shift: 
We define a new metric based on CLIP , using the intuition that Glaze succeeds if the mimicked art has been impacted enough by Glaze to be classified into a different art genre from the artist’s original artwork. We leverage CLIP model’s ability to classify art images into art genres. Given a set of mimicked art targeting an artist V, we define CLIP-based genre shift rate as the percentage of mimicked art whose top 3 predicted genres do not contain V’s original genre. A higher genre shift rate means more mimicked art belongs to a different genre from the victim artist, and thus means more successful protection.

To calculate the genre shift we use a set of 27 historical genres from WikiArt dataset and 13 digital art genres  as the candidate output labels. In Appendix A.3, we show that a pre-trained CLIP model is able to achieve high genre classification performance. We report the average CLIP-based genre shift for all 199 victim artists across all mimicked artworks.

We use CLIP-based genre shift as a supplemental metric to evaluate Glaze.
because it is only able to detect style changes at the granularity of art genres. However, mimicry attacks also fail when.
Glaze causes the mimicked artwork quality to be very low, something that CLIP cannot measure. Measuring the quality of generated image has been a challenging and ongoing research problem in computer vision .

6.3 Glaze’s Protection Performance.

Style mimicry success when Glaze is not used. 
Mimicry attacks are very successful when the mimic has access to a victim’s original (unmodified) artwork. Examples of mimicked artwork can be found in Figure 8 (more in Figure 23 in Appendix). The leftmost two columns of Figure 8 show a victim artist’s original artwork, while the third column depicts mimicked artwork generated by a style-specific model trained on victim’s original artwork when Glaze is not used. In our user study, over &gt;95% of respondents rated the attack as successful. Table 2, row 1, gives the artist-rated and CLIP-based genre shift for mimicry attacks on unprotected art.

SD models produce stronger mimicry attacks than DALL⋅E-m models, according to our user study (see Table 2). This is unsurprising, as DALL⋅E-m models generally produce lower-quality generated images. CLIP-based genre shift does not reflect this phenomenon, as this metric does not assess image quality.

Glaze’s success at preventing style mimicry. 
Glaze makes mimicry attacks markedly less successful, as shown in Figure 8. Columns 5 and 6 (from left) show mimicked artwork when the style-specific models are trained on artwork protected by.
Glaze. For reference, column 4 shows an example style-transferred artwork.
Omega (x,T) used to compute Glaze cloaks for the protected art pieces. Overall, Glaze achieves &gt;93.3% artist-rated PSR and.
&gt;96.0% CLIP-based genre shift (see Table 2). Glaze’s protection performance is slightly higher for current artists than for historical artists. This is likely because the historical artists’ images are present in the training datasets of our generic models (SD, DALL⋅E-m), highlighting the additional challenge of protecting well-known artists whose style was already learned by the generic models.

How large of perturbations will artists tolerate?
Increasing the.
Glaze perturbation budget enhances protection performance. We observe that both artist-rated and CLIP-based genre shift increase with perturbation budget (see Figure 10, Table 3, and Figure 18). Given this tradeoff between protection success and Glaze protection visibility on original artwork, we evaluate how perturbation size impacts artists’ willingness to use Glaze.

We find that artists are willing to add fairly large Glaze perturbations to their artwork in exchange for protection against mimicry. To measure this, we show 3 randomly chosen pairs of original/cloaked artwork to each of the 1,156 artists in our first study. For each art pair, we ask the artist whether they would be willing to post the cloaked artwork (instead of the original, unmodified version) on their personal website. More than 92% of artists select “willing” or “very willing” when p=0.05. This number only slightly increases to 94.3% when p=0.03. Figure 10 details artists’ preferences as perturbation budget increases. (see Figure 11 for examples of cloaked artwork with increasing p). Based on these results, we use perturbation budget p=0.05 for all our experiments, since most artists are willing to tolerate this perturbation size.

Surprisingly, over 32.8% artists are willing to use cloaks with p=0.2, which is clearly visible to human eye (see Figure 11, high resolution version in appendix Figure 20). While we are surprised by this high perturbation tolerance, in our follow-up free response artists noted that they would be willing to tolerate large perturbations because of the devastating consequence if their styles are stolen. One participant stated that “I am willing to sacrifice a bit image quality for protection.” Many artists (&gt;80%) also noted that they have already used traditional, more visually disruptive techniques to protect their artwork online when posting online, that is,adding watermark or reducing image resolution. One participant stated that “I already use low to medium resolution images only for online posting, thus this would not impact my quality control too much.”

6.4 Glaze’s Protection Robustness.

Next, we test Glaze’s efficacy in more challenging scenarios. First, we measure performance when the mimic uses a different feature extractor for mimicry than the one used by the artist to generate the cloak. Second, we measure what happens when the mimic has uncloaked artwork samples from the victim. Due to the poor mimicry performance of DALL⋅E-m, we focus our evaluation using SD as the generic model.

Artist/mimic use different feature extractors. 
In the real world, it is possible that the mimic will use a different model (and thus a different image feature extractor) for style mimicry than the one used by the victim artist to cloak their artwork. While the feature extractors may still be similar because of the well-known transferability property between large models , their differences could reduce the efficacy of cloaking. We test this scenario using three feature extractors—Phi -A, Phi -B, and.
Phi -C. Phi -A and Phi -B have different model architectures (autoencoder-KL  vs. VQ-VAE ) but are both trained on the ImageNet dataset . Phi -A and Phi -C have different model architectures (autoencoder-KL vs VQ-VAE) and training datasets (ImageNet vs. CelebA ).

In our experiments, the victim artist uses one feature extractor (either.
Phi -B or Phi -C) to optimize cloaked artwork, and the mimic trains their style-specific models with SD models using Phi -A. Despite the difference in victim/mimic extractors, Glaze’s protection remains highly successful (left half of Figure 12)—the style of mimicked artwork remains distinct from artist’s true style. Artist-rated and CLIP-based genre shift measurements confirm this observation. Artist-rated PSR is.
&gt;90.2%, while CLIP-based genre shift is &gt;94.0%. The PSR is slightly higher when the two feature extractors only differ in architectures (Phi -B to.
Phi -A) than when they differ in both architecture and training data (Phi -C to Phi -A).

Mimic has access to uncloaked artwork. 
Another challenging scenario is when the mimic gains access to some uncloaked artwork from victim artists. This is a realistic scenario for many prominent artists with a large online presence. As expected, Glaze’s protection performance decreases when the mimic has access to more uncloaked artwork (right side of Figure 12). As the ratio of uncloaked/cloaked art in the mimic’s dataset increases, the mimicked artwork becomes more similar to artist’s original style. Yet, Glaze is still reasonably effective (87.2% artist-rated PSR) even when artists can only cloak 25% of their artwork. This validates our hypothesis in §5.4 that cloaking will have a noticeable effect as long as the mimic has some cloaked training data.

A mimic with access to a large amount of uncloaked artwork is still an issue for Glaze. Fortunately, in our user study, we found that 1) many artists constantly create and share new artwork online, which can be cloaked to offset the percentage of uncloaked artwork, and 2) many artists change their artistic style over time. In our user study, we asked artists to estimate the number of unique art pieces they currently have online (M) and the estimated number of art pieces they anticipate uploading each subsequent year (Y). Among artists with an existing online presence, over 40% have.
Y/M&gt;25%, meaning that one year from now, &gt;20% of their total online artwork would be cloaked (if they start using Glaze.
immediately). More than 81% of artists also stated that their art style has changed over their career, and half of these said that theft of their old, outdated styles is less concerning.

6.5 Real-World Performance.

Next, we test Glaze against a real-world style mimicry-as-a-service system, scenario.gg. Scenario.gg is a web service that allows users to upload a set of images in a specific style. The service then trains a model to mimic the style and returns an API endpoint that allows the user to generate mimicked images in the trained style. The type of model or mimicry method used by the service is unknown.

Glaze remains effective against scenario.gg. We ask.
scenario.gg to mimic the style from a set of cloaked or uncloaked artwork from 4 current artists and 19 historical artists. Table 4 shows that when no protection is used,
scenario.gg can successfully mimic the victim style (&lt; 7.2% protection success). The mimicry success of scenario.gg is lower than our mimicry technique, likely because scenario.gg trains the model for fewer iterations due to computational constraints. When we use.
Glaze to cloak the artwork and upload the cloaked artwork,
scenario.gg fails to mimic the victim style (&gt;92.1% artist-rated PSR and &gt;93.9% CLIP-based genre shift rate) as shown in Table 4. 7. Countermeasures.

We consider potential countermeasures a mimic could employ to reduce the effectiveness of Glaze. We consider the strongest adversarial setting, in which the mimic has white-box access to our protection system, that is,access to the feature extractor used and protection algorithm. In our experiments, we assume the mimic uses the SD model as the generic model and test the efficacy of each countermeasure on the 13 victim artists from §6.2. Here, we focus on artist-rated PSR metric, because many countermeasures trade off image quality for mimicry efficacy, and CLIP-based metric does not consider image quality.

Image transformation. 
A popular approach to mitigate the impact of small image perturbations, like those introduced by Glaze, is to transform training images before using them for model training . In our setting, the mimic could augment the cloaked artwork before fine-tuning their model on them to potentially reduce cloak efficacy. We first test Glaze’s resistance to two popular image transformations, adding Gaussian noise and image compression. We also consider a stronger version of this countermeasure that then tries to correct the image quality degradation introduced by the transformations.

Transforming cloaked artwork does not defeat Glaze’s protection. Figure 13 shows that as the magnitude of Gaussian noise (sigma ) increases, the quality of mimicked artwork decreases as fast as or faster than cloak effectiveness. This is because models trained on noisy images learn to generate noisy images. We observe a similar outcome when mimic uses JPEG compression (Figure 14), where image resolution and quality degrade due to heavy compression. Artists-rated PSR decreases slightly but remains above &gt;87.4% across both types of data transformations. Artists consider Glaze’s protection to be successful when mimicked artwork is of poor quality.

Radiya et al.robust training.
Radiya.
et al.design a robust training method to defeat cloaking tools like Fawkes  and Lowkey  in the face recognition setting. At a high level, this method augments the attacker’s training dataset with some cloaked images generated by the cloaking tool and the correct output labels. Training on such data makes the model more robust against cloak perturbations on unseen cloaked images at inference time, and thus, can potentially circumvent the protection.

We test if this robust training approach can defeat Glaze. We assume the mimic first robustly trains the feature extractors in their generic models using cloaked artwork generated by Glaze, and then trains the generator model to generate images from the robust feature space. Finally, the mimic uses the robust generic model for style mimicry as in §6. We discuss the detailed robust training setup in AppendixLABEL:app:counter.

Glaze performance remains high, even if the mimic robustly trains the generic model for many iterations before using it for style mimicry (see Figure 15). As the model becomes more robust, the mimicked artwork is less impacted by cloaking (less influenced of the target style). However, robust training greatly degrades mimicked image quality, preventing successful mimicry. Overall, the artist-rated PSR remains.
&gt;88.7%.

As discussed in §5.4, Glaze remains reasonably effective against Radiya et al.because 1) the continuous output space of the generative model, and 2) high quality requirement of art generation. Robust training reduces cloaking’s effectiveness but cannot completely remove its impact. In the classification case (facial recognition), this reduced effectiveness only manifests in small changes in classification confidence (compared to no cloaking) and often does not change the discrete classification outcome. However, in the context of generator models, the continuous output space means that even less-effective cloaks still directly affect the mimicked artwork. Combined with the high quality requirement, the reduced protection effect is enough to disrupt style mimicry, as shown in Figure 15. Additional robust training simply degrades generation quality, rather than reducing cloaking efficacy.

8. Discussion.

Here we conclude with a discussion of the limitations of the current system, and outline plans for ongoing work.

Limitations.
The first limitation of our approach is that protection relies on artists cloaking a portion of their art in the mimic model’s training dataset. This is particularly challenging for established artists because 1) their styles have matured over the years and are more stable, and 2) many of their art pieces have already been downloaded from art repositories like ArtStation and DeviantArt. Someone can simply mimic these artists’ style using only old artwork collected before the release of.
Glaze. While it is a win for the artists in terms of preventing mimic from training on newer artwork, these artists must rely on data collectors providing an opt-out and removal option in order to stop style mimicry.

Second, a system like Glaze that protects artists faces an inherent challenge of being future-proof. Any technique we use to cloak artworks today might be overcome by a future countermeasure, possibly rendering previously protected art vulnerable. While we are under no illusion that.
Glaze will remain future-proof in the long run, we believe it is an important and necessary first step towards artist-centric protection tools to resist invasive AI mimicry. We hope that Glaze and followup projects will provide some protection to artists while longer term (legal, regulatory) efforts take hold.

Ongoing Work.
We are actively developing Windows and Mac versions of.
Glaze for use for members of the artist community. We also recognize that a significant portion of the artist community may lack access to these tools because of lack of awareness or computational resources. We are actively exploring approaches for more proactive art mimicry protection for groups of artists that do not require individual effort.
