LayerDiffusion: Layered Controlled Image Editing with Diffusion Models.

Abstract.

Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods.
LayerDiffusion opens up new possibilities for controllable image editing.

1. Introduction.

Given a single image of your pet, it can be imagined embarking on a worldwide journey and performing specific actions in any location. Generating such an image is a challenging and fascinating task in image editing. It entails preserving the specific subject’s unique characteristics in new backgrounds and ensuring their seamless integration into the scene, harmoniously and naturally, while simultaneously accommodating multiple editing actions.

Recently, significant progress has been made in the development of deep learning-based large-scale text-to-image models . These models can generate high-quality synthetic images based on text prompts, enabling text-guided image editing and producing impressive results. As a result, numerous text-based image editing methods  have emerged and evolved. However, such models cannot mimic specific subject characteristics. Even with the most detailed textual descriptions of an object, they may generate instances with different appearances and still struggle to maintain background consistency. Thus, the current leading image editing methods encounter several challenges, including rigid editing limited to specific domain images , the inability to simultaneously edit both the background and specific subjects, and the requirement for additional auxiliary input information . These issues hinder the advancement of controllable image editing.

In this paper, we propose a semantic-based layered controlled image editing method, which we call LayerDiffusion, to alleviate these issues. By simply inputting textual descriptions of multiple editing actions, along with the target image and a reference image, we can perform non-rigid editing and attribute modification of specific subjects, generating images consistent with the textual descriptions while maintaining the consistency of the specific subject and background features with the input image. As shown in Figure 1, we can make a dog jump in a forest or a giraffe lies on a beach or modify their shapes and attributes in the original scene.

To implement our method, we leverage the robust and high-quality image generation capabilities of a large-scale text-to-image model . Our method comprises a well-defined sequence of steps. Initially, we utilize a mask to eliminate interference from foreground objects effectively. Subsequently, we apply a layered controlled optimization strategy to optimize the text embeddings acquired from the text encoders , following the segmentation of the target text. This process aims to generate image backgrounds that exhibit a remarkable similarity to the reference images. Next, we employ a layered diffusion training strategy to fine-tune the model, thereby augmenting its ability to preserve the similarity between the specific subjects, backgrounds, and input images. Finally, during the diffusion process with the fine-tuned model, we adopt an iterative guidance strategy, where a highly constrained text embedding is iteratively employed to denoise the images. Consequently, this generates a final image aligning with the textual description.

We emphasize the contributions of each component in our method through ablation studies and compare our approach with other relevant image editing methods , clearly demonstrating superior editing quality. Furthermore, we conduct a user study to subjectively evaluate the quality of the images generated by our method, which aligns most closely with human perception. We summarize our main contributions as follows:

1. We propose LayerDiffusion. To the best of our knowledge, this is the first image editing method that enables simultaneous editing of specific subjects and backgrounds using a single input image.
2. We introduce a novel layered diffusion training framework that enables arbitrary and controllable editing of specific subjects and backgrounds.
3. Experimental results demonstrate that our method generates images with highly similar features to the input images.

2. Related Work.

Image synthesis has recently made significant advancements . With the development of diffusion models  in image processing tasks , new text-guided solutions have emerged in the field of image editing and produced impressive results . The powerful generative capabilities of diffusion models enable the generation of numerous high-quality images. Consequently, many image editing tasks no longer require training a large-scale text-to-image model, as pre-trained models can be used for image editing based on textual descriptions. Diffusion models have tremendous potential for image editing tasks guided by text descriptions. Many studies  have utilized pre-trained models as generative priors, which can be categorized into two approaches: training-free and fine-tuned method. SDEdit  introduces intermediate noise to an image, which can be augmented with user-provided brush strokes, followed by denoising through a diffusion process conditioned on the desired edit. P2P  and PnP  utilize cross-attention or spatial features to edit both global and local aspects of an image by directly modifying the text prompt. However, they often preserve the original layout of the source image and struggle with non-rigid transformations.

Fine-tuned methods  have also shown remarkable performance. DiffusionCLIP  leverages the CLIP  model to provide gradients for producing impressive style transfer results. Textual-inversion  and Dreambooth  fine-tune the model using multiple sets of personalized images, resulting in the synthesis of images depicting the same object in new environments. Imagic  fine-tunes the model by optimizing text embeddings and achieves image editing through linear interpolation of text embeddings.

Similarly, our approach leverages target text descriptions to fine-tune the model and enable various image editing operations. Dreambooth  and Imagic  are methods that resemble our approach. However, Dreambooth requires multiple input images and often fails to produce satisfactory results when dealing with a single image. Imagic, on the other hand, faces challenges in simultaneously performing multiple editing actions, such as editing both the background and specific subjects simultaneously. In contrast, our method allows for simultaneous editing of specific subjects and the background using only a single input image.

3. Method.

3.1 Preliminaries.

Stable Diffusion Models (SDM) is a publicly available text-to-image diffusion model trained on the LAION-5B  dataset. Instead of directly operating in the image space, SDM is based on the latent diffusion method, which means the forward and reverse diffusion sampling are operated in the latent space. Given the trained autoencoder, the image p is convert to low-dimensional latent variable x at each timestep t. SDM also introduces an important modification in the form of text-based conditioning. During the denoising process, SDM can be conditioned on an additional input vector, which is typically a text encoding produced by a pre-trained CLIP text encoder P. Specially, the P extract words from a given text prompt y and convert them into tokens, denoted by e=tau ϕ(y). These tokens are further transformed into text embeddings, which are used to condition the neural network during the training process:

Consequently, SDM facilitates the generation of images based on textual input by employing reverse diffusion sampling in the latent space. Instead of relying on ϵtheta (xt,t), the model utilizes a text-conditioned neural network denoted as ϵtheta (xt,t,tau ϕ(y)). We implement the proposed approach in this work by fine-tuning this pre-trained model.

3.2 Layered Diffusion.

Our approach leverages target text descriptions to facilitate a wide range of image editing actions, including object size, property modifications, and background replacement while preserving specific subject details closely tied to the original image. To achieve this, we fine-tune a state-of-the-art diffusion model . Furthermore, we introduce a layered editing method for the background and specific foreground objects. As illustrated in Figure 2, our method begins by separating the background. We apply a layered controlled optimization strategy to refine the segmentation text embeddings acquired from the text encoders, which come from the target text.Then we identify the optimal text embedding that aligns with the desired target background in proximity to the target text embedding. Subsequently, we employ a layered diffusion strategy to fine-tune the diffusion model. This approach enhances the model’s capability to maintain similarity between specific subjects, backgrounds, and input images, allowing for finer control and precision in image editing through parameter adjustments. During the inference stage, we utilize an iterative guidance strategy to directly generate images that align with the multiple image editing actions described in the input text without the text embedding interpolation. Each step of the process is outlined in detail below.

3.2.1 Layered controlled optimization.

Due to the potential interference of multiple text descriptions, optimizing text embeddings can be unstable during image editing. As a result, previous methods for image editing have often struggled to effectively modify selected object property and backgrounds simultaneously.

To this end, we aim to separate the background and foreground to reduce interference between different textual information. The target text T is first fed into the Stable Diffusion model  to obtain the target image Ot. Then T is decomposed into Ta and Tb, which describe object properties and background separately and sent to the text encoder  to output the corresponding text embeddings ea in RC×N and eb in RC×N, where C is the number of tokens, and N is the token embedding dimension. However, ea and eb are in the distant embedding space, so we cannot directly perform linear interpolation on them. To make ea and eb match our input image background as much as possible and be in a close embedding space, we freeze the parameters of the diffusion model and optimize ea and eb simultaneously using the diffusion model objective . In fact, we can optimize the initial text embedding to make it closer to the target image (modify the background) space or reference image (modify object properties) space. This process is controlled by the object mask M and can be represented as follows:

where M is computed by Segment Anything Model (SAM) , and xt is the noisy version of the input image, and ftheta means the forward diffusion process using pre-trained diffusion model. The optimized text embeddings make it meaningful to modify the linear interpolation weights of ^ea and ^eb as follows:

according to the experimental analysis of text embedding interpolation in Imagic , we tend to set the weight alpha that describes object properties to 0.7. 3.2.2 Model fine-tuing.

We obtain new text embeddings eopt by linearly interpolating multiple optimized text embeddings. Due to the limited number of optimization steps, the resulting embeddings may not lead to a consistent representation of the selected objects or background in the input image. Therefore, we propose a layered diffusion strategy to optimize model parameters while freezing the optimized text embeddings eopt. This enables the model to fit the desired image at optimized text embedding points. To achieve the arbitrary modification and combination of foreground object properties and backgrounds, we employ SAM  to derive Mt (object) and 1−Mt (background) from Ot and subsequently obtain Mr (object) and 1−Mr (background) from the reference image Or. The aforementioned can be achieved by optimizing the following equations:

The total loss can be represented as follows:

This approach enables us to manipulate the foreground object and background independently, allowing for precise control over the final output image.

3.2.3 Iterative guidance strategy.

We first represent the diffusion process of a pre-trained model as follows:

where D represent an update process: I×C -> I, I in RH×W×C is the image space, and C is the condition space, and y in C is a text prompt. From T to 0, IT gradually changes from a Gaussian noise distribution to a desired image by y. Nonetheless, due to the significant gap between the initial image and the desired image in our task, applying the base generative diffusion process with fine-tuned models under condition y(that is,eopt) may still result in failures in modifying object properties in sometimes, such as modifications of actions.

This issue in image editing is due to the lack of a strong constraint corresponding to the text description of the edited attributes in the diffusion process. The network bias leads the diffusion model to favor object properties in the initial image. To address this, we strengthen the object properties by utilizing the decomposed ^ea in the diffusion process. Specifically, we perform the following approach:

3.3 Implementation details.

We adopt the stable diffusion text-to-image model  as the baseline for our method. Specifically, we utilize the publicly available v1.4 version of the model, which was pre-trained on the LAION-5B dataset  and built upon the latent diffusion model. We first fine-tune the text embeddings with a learning rate of 1e-3 using Adam  and perform 500 steps in most of our experiments. Subsequently, we fine-tune the diffusion model itself, using a learning rate of 2e-6 and executing 250 steps. We employ an iterative guidance strategy throughout the diffusion process, starting from random noise. This iterative process consist of 50 iterations by default, resulting in more refined results. For one image, it takes about 2 minutes to run on a single NVIDIA A100 GPU.

4. Experiments.

4.1 Qualitative Evaluation.

We extensively evaluate our approach using images from various domains and categories. Our method involves a simple text prompt-based editing process, allowing for tasks such as background replacement and object property modification. The images utilized in our experiments are copyright-free on the Internet. We employ a layered editing strategy to ensure robustness and controllability in the editing process. This approach enables multiple editing actions simultaneously on the images, demonstrating excellent editing controllability. The probabilistic diffusion model also motivates us to test our method under different random seeds. By employing our layered diffusion strategy, we can generate images that closely match the provided text descriptions while preserving the critical attributes of the original image in most cases. Our method produces multiple editing results from a single text prompt, providing users with a selection of options to choose from.

In Figure 3, we present some edited images. These images preserve the distinct characteristics of the input image, and they are altered based on text prompts to accommodate a range of editing actions that go beyond mere background replacement and property modification. Our method can execute precise editing actions on the images by leveraging reference background or foreground objects. For instance, we can alter foreground objects based on reference foreground object maps or implement background modifications guided by reference background maps. More results can be found in the supplementary material.

4.2 Comparisons.

We primarily compare our proposed image editing method with previous text prompt methods, such as SDEdit , Imagic , and PnP . It is worth noting that Imagic  necessitates fine-tuning of both the network and text embeddings, while our method adopts a similar fine-tuning approach.

As shown in Figure 4, non-rigid editings, such as jumping and rotation, have significant challenges in image editing tasks. This complexity leads to the failure of both PnP  and SDEdit  in performing editing actions. Additionally, Imagic  tends to produce overfitting of the original image and text embeddings during training, thereby making accurate image editing difficult, especially when modifying text prompts go beyond attribute editing and involve adding other editing actions, such as foreground-background editing simultaneously. In contrast, our approach adopts a layered strategy that allows for the simultaneous execution of multiple editing actions. As a result, our method achieves impressive results in real image editing tasks. The last two columns of Figure 4 show the edited results generated by employing different random seeds. Our method outperforms others in multitask editing performance.

In Figure 4, we generate a reference background image from the diffusion model , and our layered diffusion approach allows us to make the edited image as close as possible to the reference background image. We can also choose our reference image as long as it is close to the perspective of the original image. We show more results in the supplementary material.

Text-based image editing methods are a relatively new direction, and there is currently no standard benchmark for evaluating our approach. Although Imagic  propose the TEdBench, it includes only a single non-rigid edit, which is also not fully applicable to our approach. To further assess the quality of our generated results, we utilize the TEdBench dataset to generate over 300 images per method for a preliminary evaluation. The supplemental material includes the used text prompts. We use a CLIP-based text-image clip score , which measures the cosine similarity between the text prompt and the image embeddings. As our method aims to maintain proximity between selected object features before and after non-rigid editing, the CLIP score does not effectively demonstrate the superiority of our approach (see Figure 5 (f)-(g) and Table 1 (f)-(g)). However, it still partially reflects the state of our image editing, such as changes in the motion of the selected object.  Table 1 provides an approximate representation of the CLIP scores for several methods, and our approach achieves the highest score.

4.3 Ablation Study.

In this section, we present a comprehensive analysis of the three modules employed in our method. We utilize the TEdBenchdataset and generate over 300 images using 20 different random seeds. As an auxiliary objective evaluation metric, we employ the text-image CLIP score , which is presented in Figure 7. Furthermore, we present the specific performance of each component in Table 1. As mentioned previously, the CLIP score may not fully capture the suitability of our method as it primarily focuses on the alignment between images and text. For instance, the results of (b), (g), and (i) show high CLIP scores, but their object features significantly differ from the reference images.

As shown in Figure 5 and Table 1, (a) does not utilize Lobj, resulting in a background that matches the reference image, while the properties of the foreground objects differ substantially. On the other hand, (b) demonstrates that Lbg preserves a more similar background. (c), (d), and (e) analyze the impact of different weights assigned to the two losses, which affect the similarity of the background and foreground objects. In this paper, we mostly set lambda 1 to 2 and lambda 2 to 1, except when lambda 1 is set to 3 for smaller foreground objects. (g), (h), and (i) validate the effectiveness of each of the three modules in our method. (g) enhances the similarity of the background, (h) controls the global features, and (i) significantly increases the percentage of image generation results that satisfy the description text, rising from 43% to 81%.

4.4 User Study.

Furthermore, we conduct a user study to evaluate and compare the subjective perception of our method with several other approaches. To ensure a fair comparison, we randomly select ten generated images and utilize two random seeds to generate our results. We present two discriminant conditions for each image: background similarity and action similarity. We then ask 20 participants to rate the resulting images on a scale ranging from 1 to 5, with a rating of 5 indicating a very good match and 1 indicating a very poor match. The histogram on the right-hand side of  Figure 7 shows the average scores. Remarkably, our method achieves optimal subjective performance compared to the other methods.

5. Limitations.

While our approach demonstrates superior performance in achieving controlled image editing and accomplishes remarkable results in tasks involving multiple editing actions, it is essential to acknowledge three significant challenges. (1) Dealing with fine-grained tasks is still challenging for our method while we rely on a pre-trained text-to-image diffusion model and the problem of overfitting that occurs during model fine-tuning. Figure 6 demonstrates that our method will produce artifacts when confronted with textures with intricate details or facial features. (2) As shown in the Figure 6, another challenge arises when there is a notable disparity in camera angles between the input reference image and the desired edited image, leading to the creation of visually inconsistent scenes. This limitation can be mitigated by incorporating additional descriptions about the camera position in the target text. (3) We need to fine-tune the model to accommodate the reference image. Appropriately fine-tuning specific parameters may be required for unconventional or atypical manifestations to generate good results in sometimes.

6. Conclusion.

We propose LayerDiffusion, a semantic-based layered image editing method that simultaneously edits specific subjects and backgrounds using a single input image. LayerDiffusion preserves the unique characteristics of the subjects while integrating them seamlessly into new scenes. Extensive experimentation demonstrates that our method generates images closely resembling the feature of the input images, surpassing existing approaches in editing quality and controllability. User studies confirm the subjective perception of the generated images, aligning with human expectations. Our contributions include introducing LayerDiffusion as the first method for simultaneous editing of specific subjects and backgrounds. We develop a layered diffusion training framework for controllable image editing, which opens up new possibilities for text-guided image editing tasks. We may focus on preserving complex textures and facial features in the future.
