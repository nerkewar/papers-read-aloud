Understanding and Mitigating Copying.
in Diffusion Models.

Abstract.

Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set. Code is available at https://github.com/somepago/DCR.

aboxcolback=lightroyalblue,colframe=black.

1. Introduction.

A major hazard of diffusion models is their ability to produce images that replicate their training data, often without warning to the user. Despite their risk of breaching privacy, data ownership, and copyright laws, diffusion models have been deployed at the commercial scale by subscription-based companies like midjourney, and more recently as offerings within search engines like bing and bard. Currently, a number of ongoing lawsuits are attempting to determine in what sense companies providing image generation systems can be held liable for replications of existing images.

In this work, we take a deep dive into the causes of memorization for modern diffusion models. Prior work has largely focused on the role of duplicate images in the training set. While this certainly plays a role, we find that image duplication alone cannot explain much of the replication behavior we see at test time. Our experiments reveal that text conditioning plays a major role in data replication, and in fact test-time replication can be greatly mitigated by diversifying captions on images, even if the images themselves remain highly duplicated in the training set. Armed with these observations, we propose a number of strategies for mitigating replication by randomizing text conditioning during either train time or test time. Our observations serve as an in-depth guide for both users and builders of diffusion models concerned with copying behavior.

2. Related work.

Memorization in generative models. 
Most insights on the memorization capabilities of generative models are so far empirical, as in studies by for GANs and a number of studies for generative language models.

Recently, investigated data replication behaviors in modern diffusion models, finding 0.5-2% of generated images to be partial object-level duplicates of training data, findings also mirrored in. Yet, what mechanisms lead to memorization in diffusion models, and how they could be inhibited, remains so far uncertain aside from recent theoretical frameworks rigorously studying copyright issues for image duplication in.

Removing concepts from diffusion models. Mitigations deployed so far in diffusion models have focused on input filtering. For example, Stable Diffusion includes detectors that are trained to detect inappropriate generations. These detectors can also be re-purposed to prevent the generation of known copyrighted data, such as done recently in midjourney, which has banned its users from generating photography by artist Steve McCurry, due to copyright concerns. However, such simple filters can be easily circumvented , and these band-aid solutions do not mitigate copying behavior at large. A more promising approach deletes entire concepts from the model as in and , yet such approaches require a list of all concepts to be erased, and are impractical for protecting datasets with billions of diverse images covering many concepts.

3. How big of a problem is data replication?

and have shown that diffusion models can reproduce images from their training data, sometimes to near-perfect accuracy. However, these studies induce replication behavior by prompting the model with image captions that are directly sampled from the LAION dataset – a practice that amplifies the rate of replication for scientific purposes. We study the rate at which replication happens with real-world user-submitted prompts. This gives us a sense of the extent to which replication might be a concern for typical end-users.

We randomly sample 100K user-generated captions from the DiffusionDB  dataset and generate images using Stable Diffusion 2.1. We use SSCD features to search for the closest matches against these images in a subset of the training dataset. Note that SSCD was found to be one of the best metrics for detecting replication in , surpassing the performance of CLIP. We compare each generated image against ∼400 million images, roughly 40% of the entire dataset.

We find that ∼1200 images (1.2%) have a similarity score above 0.5, indicating these may be duplicates. Note that this is likely an underestimation of the true rate, as our SSCD-based search method is unlikely to find all possible matches. We show several of these duplicates in Figure 1. Sometimes, the captions precisely describe the image content (for example, ‘Jake Gyllenhaal’). However, we also discover instances where captions do not mention the content from the generated image. For example, in the first duplicate, the user caption is “flower made of fire, black background, art by artgerm”, and the LAION caption is “Colorful Cattleya Orchids (2020) by JessicaJenney”. Several duplicates identified by SSCD also have high similarity scores due to the simple texture of the generated image. We include these SSCD false positives and other examples in the Appendix.

4. Experimental Setup.

A thorough study of replication behavior requires training many diffusion models. To keep costs tractable, we focus on experiments in which large pre-trained models are finetuned on smaller datasets. This process reflects the training of Stable Diffusion models, which are pre-trained on LAION and then finetuned in several stages on much smaller and more curated datasets, like the LAION Aesthetics split.

Datasets:

We use Imagenette ([1] https://github.com/fastai/imagenette), which consists of 10 classes from Imagenet  as well as two randomly sampled subsets of 10,000 and 100,000 images from LAION-2B  for our experiments. The LAION subsets, which we denote as LAION-10k and LAION-100k, include captions, while the Imagenette data is uncaptioned. For some experiments, we use BLIP v1  to generate captions for images when needed. We provide details for LAION subset construction in the Appendix.

Architecture &amp; Training:

We use the StableDiffusion-v2.1 checkpoint as a starting point for all experiments. Unless otherwise noted, only the U-Net  part of the pipeline is finetuned (the text and auto-encoder/decoder components are frozen) as in the original training run, and we finetune for 100000 iterations with a constant learning rate of 5e−6 and 10000 steps of warmup. All models are trained with batch size 16 and image resolution 256. We conduct evaluations on 4000 images generated using the same conditioning used during model training. Refer to Appendix for complete details on the training process.

Metrics:

We use the following metrics to study the memorization and generation quality of finetuned models. Frechet Inception Distance (FID)evaluates the quality and diversity of model outputs. FID measures the similarity between the distribution of generated images and the distribution of the training set using features extracted by an Inception-v3 network. A lower FID score indicates better image quality and diversity.

quantify copying in diffusion models using a similarity score derived from the dot product of SSCD representations of a generated image and its top-1 match in the training data. They observe that generations with similarity scores greater than 0.5 exhibit strong visual similarities with their top-1 image and are likely to be partial object-level copies of training data.

Given a set of generated images, we define its dataset similarity score as the 95-percentile of its image-level similarity score distribution. Note that measuring average similarity scores over the whole set of generated images is uninformative, as we are only interested in the prevalence of replicated images, which is diluted by non-replicated samples. For this reason, we focus only on the similarity of the 5% of images with the highest scores.

5. Data Duplication is Not the Whole Story.

Existing research hypothesizes that replication at inference time is mainly caused by duplicated data in the training set. Meanwhile, data replication has been observed in newer models trained on de-duplicated data sets . Our goal here is to quantify the extent to which images are duplicated in the LAION dataset, and understand how this impacts replication at inference time. We will see that data duplication plays a role in replication, but it cannot explain much of the replication behavior we see.

5.1 How Much Duplication is Present in LAION?

In order to understand how much data duplication affects Stable Diffusion, we first identify clusters of duplicated images in LAION. We use 8M images from LAION-2B for our duplication analysis. Since duplicates are often duplicated many times, even a small subset of LAION is sufficient to identify them. Note that we do not use a random subset, but instead a consecutive slice of the metadata. Since the metadata is structured to provide URLs from similar domains together, this makes us likely to find large clusters.

We use SSCD to compute the 8M×8M similarity matrix between all images within a slice of metadata. We threshold the similarity scores, only keeping ones above 0.7 to identify images that are nearly identical. Then, we identify each connected component in the sparse similarity matrix and interpret it as representing a duplicated image. We show several of these clusters in the Appendix. We only consider clusters with at least 250 samples to ensure the images are duplicated enough times, which leaves ∼50 clusters with a total of around 45K images.

To measure how similar captions are within a cluster, we first compute CLIP text features. Then, we compute the similarity between captions using the dot product of CLIP text features multiplied by the unigram-Jaccard similarity between the captions of a cluster and finally compute the median value. We use the unigram-Jaccard similarity in addition to CLIP, since it better captures word-level semantics. We select 40 captions from each cluster and feed them to Stable Diffusion. Finally, we determine if the generated images are replicas of their source cluster again using SSCD.

Results are shown in Figure 2. We observe that Stable Diffusion v1.4 shows high pixel-level replication (reflected in SSCD scores &gt; 0.4) for some duplicated clusters, but only when caption similarity within the cluster is also high. In contrast, since the dataset for Stable Diffusion v2.1 was de-duplicated before training, it shows nearly no test-time replication for our identified clusters. Despite being de-duplicated, Stable Diffusion v2.1 still exhibits memorization when scanning through a larger portion of the dataset, as we observed previously in Figure 1, showing that replication can happen even when the duplication rate in the training set is greatly reduced. In both cases, we see that image duplication is not a good predictor of test-time replication.

5.2 Controlled Experiments with Data Duplication.

We train diffusion models with various levels of data duplication factor (ddf), which represents the factor by which duplicate samples are more likely to be sampled during training. We train each model for 100k iterations and evaluate similarity and FID scores on 4000 generated samples.

Figure 3 contains results for LAION-10k and Imagenette. We observe that increased duplication in the training data tends to yield increased replication during inference. The relationship between data duplication and similarity scores is not straightforward for LAION-10k. As the duplication factor increases, similarity scores rise again until reaching a data duplication factor ddf of 10, after which they decrease. Regarding FID scores, we find that a certain level of data duplication contributes to improving the scores for models trained on both datasets, possibly because FID is lowest when the dataset is perfectly memorized.

Other Observations from the Literature.

found that unconditional diffusion models can exhibit strong replication when datasets are small, despite these training sets containing no duplicated images. Clearly, replication can happen in the absence of duplication. As the training set sizes grow (∼30k), the replication behaviors seen in vanish, and dataset similarity drops, even when the number of epochs is kept constant. One might expect this for large enough datasets. However, models trained on the much larger LAION-2B dataset do exhibit replication, even in the case of Stable Diffusion 2.1 (Figure 1) which was trained on (at least partially) de-duplicated data. We will see below that the trend of replication in diffusion models depends strongly on additional factors, related especially to their conditioning and to dataset complexity.

Takeaway: When training a diffusion model, it is crucial to carefully manage data duplication to strike a balance between reducing memorization and ensuring high-fidelity of generated images. However, controlling duplication is not enough to prevent replication of training data.

5.3 The Effect of Model conditioning.

To understand the interplay between model conditioning and replication, we consider four types of text conditioning:

1. [itemsep=2pt,topsep=-2pt,leftmargin=8mm]
2. Fixed caption: All data points are assigned the same caption, An image.
3. Class captions: Images are assigned a class-wise caption using the template An image of &lt;class-name&gt;.
4. Blip/Original captions: Each point is trained on the original caption from the dataset (LAION-10k ) or a new caption is generated for each image using BLIP  (Imagenette).
5. Random captions: A random sequence of 6 tokens is sampled from the vocabulary used to uniquely caption each image.

By varying caption scenarios, we transition from no diversity in captions (“fixed caption”) case to completely unique captions with no meaningful overlap (“random caption”) case. We again finetune on the Imagenette dataset, now using these caption types. As a baseline, we consider the pretrained Stable Diffusion model without any finetuning. Figure 4 (left) shows the dataset similarity among the baseline and models finetuned using the different caption types.

We observe that the finetuned models exhibit higher similarity scores compared to the baseline model. Furthermore, the level of model memorization is influenced by the type of text conditioning. The “fixed caption” models exhibit the lowest amount of memorization, while the “blip caption” models exhibit the highest. This indicates that the model is more likely to memorize images when the captions are more diverse. However, the model does not exhibit the highest level of memorization when using “random captions”, meaning that captions should be correlated with image content in order to maximally help the model retrieve an image from its memory.

Training the text encoder.

So far, the text encoder was frozen during finetuning. We can amplify the impact of conditioning on replication by training the text encoder during finetuning. In Figure 4 (right), we observe a notable increase in similarity scores across all conditioning cases when the text encoder is trained. This increase is particularly prominent in the cases of “blip captioning” and “random captioning”. These findings support our hypothesis that the model is more inclined to remember instances when the captions associated with them are highly specific, or even unique, keys.

Takeaway: Caption specificity is closely tied to data duplication. Highly specific captions act like keys into the memory of the diffusion model that retrieve specific data points.

5.3.1 The Impact of Caption vs. Image Duplication.

In this section, we control separately for duplication of images and duplication of their captions to better understand how the two interact.

In the case of full duplication, both the image and its caption are replicated multiple times in the training data. On the other hand, partial duplication involves duplicating the image multiple times while using different captions for each duplicate(although the captions may be semantically similar). To study the partial duplication scenario, we generate 20 captions for each image using the BLIP model for both Imagenette and LAION-10k datasets. For the full-duplication case in the LAION-10k experiments, we keep the original caption from the dataset.

We present the results on LAION-10k and Imagenette in Figure 5. We investigate how dataset similarity changes for both full and partial image-caption duplication at varying levels of duplication. Overall, our findings demonstrate that partial duplication consistently leads to lower levels of memorization compared to full duplication scenarios.

In Figure 5 (left), we compare the similarity scores of several models: the pretrained checkpoint, a model finetuned without any data duplication, a model finetuned with full duplication (ddf=5), and a model finetuned with partial duplication (ddf=5). We include dashed horizontal lines representing the background self-similarity computed between the dataset and itself. In the Imagenette case, models trained without duplication and with partial duplication exhibit dataset similarity below the baseline value, indicating a lower level of memorization. In contrast, the model trained with full duplication demonstrates higher levels of memorization compared to both the baseline and other cases. In the LAION-10k experiments, the model trained without duplication surpasses the training data similarity baseline. This observation raises the question of whether the memorization is inherited from the pretrained model, considering it is also trained on the larger LAION-2B dataset. However, when we compute the similarity scores on the pretrained checkpoint, we observe significantly lower values, indicating that the observed memorization is acquired during the fine-tuning process.

In Figure 5 (middle, right), we analyze how the similarity scores and FID scores vary at different data duplication factors (ddf) for full and partial data duplication. As the ddf increases, we observe an increase in memorization for models trained with full duplication. However, for partial duplication, dataset similarity actually decreases with increased duplication. In our previous analogy, we now have multiple captions, that is keys for each duplicated image, which inhibits the memorization capabilities of the model. However, this memorization reduction comes at the cost of a moderate increase in FID at higher duplication levels.

Takeaway: Compared to full duplication, partial duplication substantially mitigates copying behavior, even as duplication rates increase.

6. Effect of the Training Regimen.

In this section, we examine how the training process and data complexity influence the degree of memorization.

Length of training.

Training for longer results in the model seeing each data point more times, and may mimic the impacts of duplication. In Figure 6 (left), we quantify the increase in dataset similarity at different training iterations. Notably, the image-only duplication model consistently exhibits lower similarity scores across epochs, while the image &amp; caption duplication model achieves the highest values, showing that caption diversity for each duplicated image significantly slows down memorization. We show matching FID scores in Figure 6 (middle).

Quantity of training data.

demonstrate that diffusion models, specifically DDPM  models without text conditioning, exhibit a tendency to replicate the training data when trained on small datasets. In this study, we extend this analysis to text-conditioned models. To conduct this investigation, we now randomly sample a subset of 100,000 images from LAION-2B. We then train using different fractions of this dataset, keeping other hyperparameters constant including the number of training iterations. The results of this experiment are found in Figure 6 (right). We observe that increasing the quantity of training data generally leads to lower similarity and FID scores. This aligns with the intuition that a larger and more diverse training set allows the model to generalize better and produce more varied outputs.

Image Complexity.

Throughout our experiments, we consistently observed a higher level of test-time replication in LAION-10k models compared to Imagenette models, even when trained with the same levels of data duplication. We put forth the hypothesis that this discrepancy arises due to the inherent diversity present in the LAION-10k dataset, not only in terms of images but also in terms of image complexity. To quantify image complexity, we employ two metrics. Histogram entropy measures entropy within the distribution of pixel intensities in an image, and JPEG compressibility (at JPEG quality 90), measures the size of images after compression. We resize and crop all images to the same resolution before computing these metrics.

Figure 7 (right) presents the distributions of entropy scores for LAION-10k and Imagenette. LAION-10k exhibits a wide range of data complexity, while Imagenette, predominantly comprised of complex real-world images, is characterized by higher complexity. Motivated by this observation, we explore the relationship between similarity scores and complexity of the top training point matched to each generation of a diffusion model trained on LAION-10k without any duplication. Remarkably, we discover a statistically significant correlation of −0.32/−0.29 (with p-values of 8e−98/7e−80) for the entropy and compression metrics, respectively. We provide results for other duplication settings in the supplementary material, where we also find significant correlation. We present density plots illustrating the distributions of both complexity metrics for high similarity points (with scores &gt;0.6) versus low similarity in Figure 7 (left, middle). The clear separation between the distributions, observed across both complexity metrics, strongly suggests that diffusion models exhibit a propensity to memorize images when they are “simple”.

Takeaway: Choosing the optimal length of training, quantity of training data, and training setup is a trade-off between model quality and memorization. When images do get memorized in the absence of training data duplication, they are likely to be “simple” in structure.

7. Mitigation strategies.

We have seen that model conditioning plays a major role in test-time replication, and that replication is often rare when image captions are not duplicated. Armed with this observation, we formulate strategies for mitigating data replication by randomizing conditional information. We study both training time mitigation strategies and inference time mitigation strategies. Training strategies are more effective, whereas inference-time mitigations are easily retrofitted into existing diffusion models. We experiment with randomizing the text input in the following ways:

1. [itemsep=2pt,topsep=-2pt,leftmargin=8mm]
2. Multiple captions (MC). We use BLIP to make 20 captions for each image and we randomly sample all of them (plus the original) when training. This is for train time only.
3. Gaussian noise (GN). Add a small amount of Gaussian noise to text embeddings.
4. Random caption replacement (RC). Randomly replace the caption of an image with a random sequence of words.
5. Random token replacement &amp; addition (RT). Randomly replace tokens/words in the caption with a random word, or add a random word to the caption at a random location.
6. Caption word repetition (CWR). Randomly choose a word from the given caption and insert it into a random location in the caption.
7. Random numbers addition (RNA). Instead of adding a random word that might change the semantic meaning of the caption, we add a random number from the range {0,106}.

In this section, we present the effectiveness of various mitigation strategies, summarized in Table 1. During train time, we find the multiple-caption strategy to be the most effective, which substantially increases caption diversity among duplicates. This matches our analysis in Figure 5. At inference time strategies, we find that we can still inject a shift in the text prompt through random token addition/replacement. Such a shift again disrupts the memorized connection between caption and image. We verify based on FID and Clipscore  that all mitigations have a minimal effect of model performance.

Images generated from models trained/tested with the best performing strategies can be found in Figure 8. For train mitigations we show our finetuned models, whereas for inference time we show mitigation strategies applied directly to Stable Diffusion. We show four images for each strategy, displaying the memorized image, generation from the regular model and from the mitigated model in each column. We fix the generation seed in each column. Overall, we find these mitigations to be quite effective in reducing copying behavior, even in large modern diffusion models. We refer the reader to the appendix regarding the hyperparameters used in the generation of Table 1 and the captions used for creating Figure 8. 8. Recommendations for a safer model.

Why do diffusion models memorize? In this work, we find that, in addition to the widely recognized importance of de-duplicating image data, conditioning and caption diversity also play fundamental roles. Moreover, we find in Figure 5 that memorization can decrease even when duplication increases, as long as captions are sufficiently diverse. These findings help to explain why large text-conditional models like Stable Diffusion generate copies. Finally, we collect our observations into a series of recommendations to mitigate copying.

Before training:

Identify large clusters of image data with significant object overlap, as measured by specialized copy detection models such as SSCD, and collapse them as described in Section 5.1. Likewise, clusters of caption overlap are potentially problematic, and duplicate captions can be resampled, for example using BLIP. In some cases, these clusters may need to be hand-curated because some concepts, like famous paintings or pictures of the Eiffel Tower, are acceptable duplications that the model may copy. Another avenue is to exclude images with limited complexity, see Section 6. During training:

We find training with partial duplications, where captions for duplicate images are resampled, to be most effective in reducing copying behavior, see Section 7. It may be worthwhile to use a low bar for duplication detection, as near-duplicates do not have to be removed entirely, only their captions resampled.

After training:

Finally, after training, inference-time strategies can further reduce copying behavior, see Section 7. Such mitigations could be provided either as a user-facing toggle, allowing users to resample an image when desired, or as rejection sampling if a generated image is detected to be close to a known cluster of duplications or a match in the training set.

While text conditioning has also been identified as a crucial factor in dataset memorization for the diffusion model in our analysis, it is important to acknowledge that other factors may also influence the outcome in complex ways. Therefore, we recommend that readers thoroughly evaluate the effectiveness of the proposed mitigation strategies within the context of their specific use cases before deploying them in production.

9. Acknowledgements.

This work was made possible by the ONR MURI program, DARPA GARD (HR00112020007), the Office of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).
