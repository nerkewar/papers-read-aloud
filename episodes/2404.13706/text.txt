1institutetext: 
Boston University.
11email: {vpetsiuk,saenko}@bu.edu.

Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models.

Abstract.

Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.

Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.

Project page: https://cs-people.bu.edu/vpetsiuk/arc.

1. Introduction.

Recent advances in Text-to-Image (T2I) generationÂ  have led to the rapid growth of applications enabled by the models, including many commercial projects as well as creative applications by the general public. On the other hand, they can also be used for generating deep fakes, hateful or inappropriate imagesÂ , copyrighted materials or artistic stylesÂ . Trained on vast amounts of data scraped from the web, these models also learn to reproduce the biases and stereotypes present in the dataÂ . While some legalÂ  and ethicalÂ  questions concerning image generation models remain unsolved, the scientific community is developing methods to limit their malicious utility, while keeping them open and accessible to the community.

Some recently proposed approaches, that we refer to as Concept Inhibition methods modify the Diffusion Model (DM) to â€œforgetâ€ some specified information. Given a target concept, the weights of the model are fine-tuned or otherwise edited so that the model is no longer capable of generating images that contain that concept. Unlike the post-hoc filtering methods (safety checkers) that can be easily circumvented by an adversaryÂ , these methods are designed to prevent the generation of undesired content in the first place. One of the motivating factors of this line of works is to limit the inappropriate content generation by the models, while keeping them open-source and accessible to the community. Based on the evaluation results of these works, which demonstrate a significantly reduced reproduction rate of the target concept in the generated images, the authors conclude that the model is no longer capable of generating the target concept and that such â€œerasure cannot be easily circumvented, even by users who have access to the parametersâ€. However, we demonstrate, theoretically and experimentally, that the models inhibited with existing methods still contain the infromation for reproducing the erased concept (FigureÂ 1). This information can be easily exploited by an adversary with access to compositional inference of the model, which is a weaker requirement than full access to the model weights.

Recent works explored, how a semantic concept can be constructed by specifying and composing more than one prompt in one generationÂ . We consider the implications of this compositional property in the context of concept inhibition. By using concept arithmetics, which is not available in single prompt inference, we use multiple input points to reconstruct the erased concept. Unlike prompt optimization attacksÂ  that leverage insufficiently generalized inhibition near the target concept (similar to adversarial attacks), our attacks leverage the compositional property and use the input points further away from the target. These points are sufficiently inhibited according to the design of the inhibition methods but still contain the information about the erased concept. Since the defense against these attacks has to take compositionality into consideration, our attacks cannot be mitigated by the methods that exclusively address the adversarial robustness.

Intuitive and straightfoward to implement, our proposed ARC (ARithmetics in Concept space) attacks would be readily available to an adversary, which makes them a serious threat against the presumably safe models. The attacks require black-box access to compositional inference of the model. This is the case for multi-prompting APIs which are becoming increasingly popular ([1] https://docs.midjourney.com/docs/multi-prompts, https://platform.stability.ai/docs/features/multi-prompting ), or for an adversary with full access to the model weights and code, for example if the model is open-source.

We present both theoretical grounding and empirical evidence of the attack effectiveness, and we quantitatively show that the attacks significantly increase the reproduction rates of the erased concepts. Compositional inference attacks are applicable to all safety mechanisms that modify the model locally (near a given input point). This simple alteration in the inference process may break the assumptions made by the defense mechanisms developers, or exploit the vulnerabilities considered to be minor to a larger extent.

To summarize, our main contributions are as follows:

1. 1. We are the first work to consider compositional property of Diffusion Models in the context of concept inhibition and its circumvention.


2. 2. We design novel attacks that exploit the limitations of concept inhibition methods, based on the theoretical framework we develop.


3. 3. We test our attacks against models inhibited with a variety of inhibtion methods and show that the attacks significantly increase the reproduction rates of the erased concepts.

Our work is not intended to discourage the use of the presented inhibition methods but to determine the strengths and limitations of different approaches, further define the notion of concept inhibtion, and ultimately advance the research on safe and responsible Text-to-Image generation. The proposed attacks can be used to test the robustness of the inhibition methods and to guide the choice of the inhibition method and its parameters. Our intentions do not include enabling the generation of inappropriate content, however, by the nature of red-team work, presented approach can be used for malicious purposes.

2. Related Work.

2.1 Diffusion Models.

Diffusion Model is a type of generative model that employs a gradual denoising process to learn the distribution pâ€‹(x)ğ‘ğ‘¥p(x) of the dataÂ . The diffusion model generates an image x0subscriptğ‘¥0x{0} in Tğ‘‡T steps by iteratively predicting and removing noise starting from the initial Gaussian noise sample xTsubscriptğ‘¥ğ‘‡x{T}. Noise prediction is learned to optimize the score function âˆ‡xlogâ¡pâ€‹(x)subscriptâˆ‡ğ‘¥ğ‘ğ‘¥\nabla{x}\log p(x).

Classifier guidanceÂ  enables generation conditioned on some input cğ‘c by adding a conditional score term gamma â€‹âˆ‡xlogâ¡pâ€‹(câˆ£x)ğ›¾subscriptâˆ‡ğ‘¥ğ‘conditionalğ‘ğ‘¥\gamma\nabla{x}\log p(c\mid x) with guidance scale gamma &gt;1ğ›¾1\gamma&gt;1 controlling the influence of the conditional signal. pâ€‹(câˆ£x)ğ‘conditionalğ‘ğ‘¥p(c\mid x) can be an external image classifier model predicting the class label cğ‘c. Classifier-free guidanceÂ  proposes to train the model jointly on conditional and unconditional denoising to obtain a single neural network that models both unconditional pâ€‹(x)ğ‘ğ‘¥p(x) and conditional pâ€‹(câˆ£x)ğ‘conditionalğ‘ğ‘¥p(c\mid x) distributions. In this case, the total guidance can be expressed as.

or in terms of the learned U-Net model Ïµtheta subscriptitalic-Ïµğœƒ\epsilon{\theta} that predicts the noise to be removed from xtsubscriptğ‘¥ğ‘¡x{t} at timestep tğ‘¡t and conditioned on prompt c1subscriptğ‘1c{1} ([2] Throughout, we imply that the string is embedded using CLIPÂ  textual encoder before being passed to Ïµitalic-Ïµ\epsilon.):

Latent Diffusion ModelsÂ  incorporate encoder Eğ¸E and decoder Dğ·D before and after the diffusion process, respectively. Moving the gradual denoising from image pixel space to lower dimensional encoder-decoder latent space improves convergence and running speeds.

2.2 Concept Arithmetics in Diffusion Models.

A series of recent worksÂ  has demonstrated that adding the guidance terms for multiple prompts during the diffusion process results in an image that corresponds to multiple prompts simultaneously. With the additional prompt guidance incorporated in EquationÂ 1, the updated noise prediction equation becomes.

where gamma jsubscriptğ›¾ğ‘—\gamma{j} (typically the same for all concepts) is the guidance scale for each additional prompt/concept cjsubscriptğ‘ğ‘—c{j}, and dj in {âˆ’1,1}subscriptğ‘‘ğ‘—11d{j}\in\{-1,1\} determines the direction of guidance â€” negative or positive. For example, the generation conditioned on the prompt â€œa picture of a carâ€ changes to a sports car or a bulkier-looking car by introducing concept â€œfastâ€ with positive d1=1subscriptğ‘‘11d{1}=1 or negative d1=âˆ’1subscriptğ‘‘11d{1}=-1 guidance respectivelyÂ .

We refer to the generation with N&gt;1ğ‘1N&gt;1 as Compositional Inference (CI) as opposed to Standard Inference (SI) that follows EquationÂ 1 (N=1,d1=1formulae-sequenceğ‘1subscriptğ‘‘11N=1,d{1}=1).

Negative guidance (dj=âˆ’1subscriptğ‘‘ğ‘—1d{j}=-1) minimizes the probability of the concept cjsubscriptğ‘ğ‘—c{j} in the generated image, and can be veiwed as a logical negation of the conceptÂ . This is used as an inference-time inhibition technique in Safe Latent Diffusion (SLD)Â  where a hand-crafted safety phrase that describes undesired content (for example, â€œviolence, nudity,â€¦â€) is applied with negative guidance.

2.3 Concept inhibition in Diffusion Models.

The actively developing line of research on unlearning concepts in the DM includes Erasing concepts from SD (ESD)Â , Ablating Concepts (AC)Â , Unified Concept Editing (UCE)Â , Selective Amnesia (SA)Â .

Each of these methods defines an optimization task to modify the weights of the generative model to prevent the generation of the target concept. Given the target concept, a text string ctsubscriptğ‘ğ‘¡c{t}, the optimization objective is designed to compromise modelâ€™s outputs or intermediate computations in the area of latent space defined by ctsubscriptğ‘ğ‘¡c{t}. This is accomplished in a supervised manner by providing the â€œground-truthâ€ outputs that the model should produce instead. The ground-truth outputs are constructed by using the corresponding outputs for some alternative anchor concept casubscriptğ‘ğ‘c{a} (AC, UCE, SA), or by negating the conditional guidance of ctsubscriptğ‘ğ‘¡c{t} itself (ESD). ESD, AC, SA solve the optimization task by fine-tuning the model weights using gradient descent, while UCE edits the weights directly using a closed-form solution. ESD, UCE, AC optimize the outputs of the conditional guidance part of the model Ïµtheta â€‹(xt,p,t)subscriptitalic-Ïµğœƒsubscriptğ‘¥ğ‘¡ğ‘ğ‘¡\epsilon{\theta}(x{t},p,t), while SA operates on image level.

Additional optimization configurations include selection of the weight groups to be updated; choice of concepts to preserve (by adding regularization terms to the optimization objective); the number of fine-tuning iterations.

2.4 Security mechanisms in Diffusion Models.

Security mechanisms in DM are aimed to address some high-risk aspects of the modelâ€™s operation, such as privacy, legal, or ethical concerns. Watermarking to validate the origin of the generated imagesÂ  or the diffusion model itselfÂ ; shielding personal images and artworks against diffusion-based editingÂ  or style mimicryÂ ; and unlearning a given concept (SectionÂ 2.3) are some of the recently developed security mechanisms. Assuming the role of an adversary to search for exploits in a system with the goal of improving its safety is a critical part of the development process in cybersecurity, referred to as red teaming.

Red team research recently performed in the context of diffusion models includes bypassing the SD safety checkerÂ , evading watermark detectionÂ , or poisoning the data to attack the model trained on itÂ .

Most relevant to our work, are the recently proposed methods to circumvent inhibition in diffusion models via prompt optimizationÂ . These works propose an optimization task in the token space (for example using a genetic algorithmÂ ) to make a given prompt problematic (one that results in reproduction of the inhibited concept). To generate problematic prompts, requires white-box access to the uninhibited diffusion model, requires white-box access to the encoder. These requirements significantly limit the practical applicability of the methods. These works modify the form of the prompt without modifying its content (for example modifying prompt â€œscary imageâ€ to â€œq scary imageâ€Â ). They aim to exploit imperfect generalization of inhibition in the vicinity of the target concept, that is low adversarial robustness.

We, on the other hand, focus on circumventing the inhibition by intentionally modifying the content of the prompt to get inputs, where the effect of inhibition is lower due to the presence of another concept. We use multiple inputs distanced away from the target concept to reproduce the target concept in their composition. Our approach requires no optimization, and no access to neither inhibited or uninhibited modelsâ€™ weights. It operates using compositional inference which is sometimes provided as a black-box service.

3. Compositional Inference Attacks.

The goal of our work is finding inputs that can be used in compositional inference to reproduce the target concept using the inhibited model, where the direct computation of the target guidance has been modified. We denote conditional guidance for concept cjsubscriptğ‘ğ‘—c{j} as gâ€‹(xt,cj,t)ğ‘”subscriptğ‘¥ğ‘¡subscriptğ‘ğ‘—ğ‘¡g(x{t},c{j},t), and for the ease of notation we omit xt,tsubscriptğ‘¥ğ‘¡ğ‘¡x{t},t in the arguments (we set guidance scale for all concepts to be equal, gamma j=gamma subscriptğ›¾ğ‘—ğ›¾\gamma{j}=\gamma):

Compositional property of DM is equivalent to linearity of gğ‘”g in semantic space (via CLIP embeddings), that is,
gâ€‹(sports car)=gâ€‹(car)+gâ€‹(fast).ğ‘”sports carğ‘”carğ‘”fastg(\textsc{sports car})=g(\textsc{car})+g(\textsc{fast}).
Optimization task in the inhibition works, analyzed in this paper, is constrained only on the target concept ctsubscriptğ‘ğ‘¡c{t} (or the target and a few neighboring concepts). As a consequence, this means that when inhibiting the concept â€˜sports carâ€™ it is assumed that the guindances for other concepts, such as gâ€‹(car)ğ‘”carg(\textsc{car}) and gâ€‹(fast)ğ‘”fastg(\textsc{fast}) are appropriately modified in an implicit way (through latent space). Our red team attacks are designed to challenge this assumption.

First, we provide the principles and intuition explaining the design and effectiveness of the proposed attacks in SectionÂ 3.1. We design the general attack framework in SectionÂ 3.2 and, finally, provide the attack implementations used in our experiments in SectionÂ 3.3. 3.1 Rationale behind the attacks.

Conditional guidance gâˆ—=gtheta âˆ—superscriptğ‘”subscriptsuperscriptğ‘”ğœƒg^{*}=g^{*}{\theta} of the uninhibited model is a function, parameterized by weights theta ğœƒ\theta, that maps a CLIP embedding of a string describing some concept cğ‘c to a vector in the latent space of the diffusion model. It is observed, that this function is linear for some points in semantic space:
gâˆ—â€‹(c1Â±c2)=gâˆ—â€‹(c1)Â±gâˆ—â€‹(c2),superscriptğ‘”plus-or-minussubscriptğ‘1subscriptğ‘2plus-or-minussuperscriptğ‘”subscriptğ‘1superscriptğ‘”subscriptğ‘2g^{*}(c{1}\pm c{2})=g^{*}(c{1})\pm g^{*}(c{2}),
where Â±plus-or-minus\pm denotes plus or minus operation in the semantic space.

To obtain the inhibited model g=gtheta ~ğ‘”subscriptğ‘”~ğœƒg=g{\tilde{\theta}}, prior works propose to optimize the weights theta ğœƒ\theta to match the output of the function at point ctsubscriptğ‘ğ‘¡c{t} to a given value y0subscriptğ‘¦0y{0} by minimizing some loss function â„’â„’\mathcal{L}:

We define the task of circumventing concept inhibition as computing gâˆ—â€‹(ct)superscriptğ‘”subscriptğ‘ğ‘¡g^{*}(c{t}) using only the inhibited model gğ‘”g.

We express inhibited function gâ€‹(c)ğ‘”ğ‘g(c) as a linear combination of uninhibited gâˆ—â€‹(c)superscriptğ‘”ğ‘g^{*}(c) and y0subscriptğ‘¦0y{0}:

where scalar lambda â€‹(c)ğœ†ğ‘\lambda(c) (which can be calculated by solving the equation for it) denotes the degree of modification (inhibition) at point cğ‘c. The degree of modification at every point is determined by the choice of the loss function and optimization parameters. The goal of the ideal inhibition is to achieve lambda â€‹(ct)=1ğœ†subscriptğ‘ğ‘¡1\lambda(c{t})=1, and lambda â€‹(c)=0ğœ†ğ‘0\lambda(c)=0 for all cğ‘c â€œindependentâ€ of ctsubscriptğ‘ğ‘¡c{t} (otherwise, guidance for cğ‘c is affected by y0subscriptğ‘¦0y{0}).

Hypothesis H1. Degree of modification lambda â€‹(c)ğœ†ğ‘\lambda(c) at point cğ‘c decreases as its distance from ctsubscriptğ‘ğ‘¡c{t} increases, and can be modeled as an exponential decay function:
lambda â€‹(c)=expâ¡(âˆ’|câˆ’ct|/sigma 2),ğœ†ğ‘ğ‘subscriptğ‘ğ‘¡superscriptğœ2\lambda(c)=\exp(-|c-c{t}|/\sigma^{2}), where sigma ğœ\sigma is a parameter that determines the rate of decay.

This hypothesis is based on the fact that the optimization task in EquationÂ 3 is designed to minimize the loss function only at concept ctsubscriptğ‘ğ‘¡c{t}. The modification is localized (unlike, for example, rotation of the whole space) and centered at ctsubscriptğ‘ğ‘¡c{t}, therefore the degree of modification is expected to decrease as the distance from ctsubscriptğ‘ğ‘¡c{t} increases. We develop the rationale for the attacks using the hypothesis. We show that as the distance between some arbitrary concept cdsubscriptğ‘ğ‘‘c{d} and inhibited concept ctsubscriptğ‘ğ‘¡c{t} increases, the linear combination(s) of gğ‘”g can be used to compute a vector colinear with gâˆ—â€‹(ct)superscriptğ‘”subscriptğ‘ğ‘¡g^{*}(c{t}). Proofs can be found in the supplementary.
Proposition P1. If |cdâˆ’ct| -> +âˆ -> subscriptğ‘ğ‘‘subscriptğ‘ğ‘¡|c{d}-c{t}|\to+\infty and gâˆ—â€‹(ctÂ±cd)=gâˆ—â€‹(ct)Â±gâˆ—â€‹(cd)superscriptğ‘”plus-or-minussubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘plus-or-minussuperscriptğ‘”subscriptğ‘ğ‘¡superscriptğ‘”subscriptğ‘ğ‘‘g^{*}(c{t}\pm c{d})=g^{*}(c{t})\pm g^{*}(c{d}), then.

where -> -> \to denotes convergence in the limit.
For a sufficiently distant concept cdsubscriptğ‘ğ‘‘c{d}, the left-hand side, which uses only the inhibited model, approaches the guidance vector gâˆ—â€‹(ct)superscriptğ‘”subscriptğ‘ğ‘¡g^{*}(c{t}) of the original model.
Proposition P2. If |cdiâˆ’ct| -> +âˆ -> subscriptsuperscriptğ‘ğ‘–ğ‘‘subscriptğ‘ğ‘¡|c^{i}{d}-c{t}|\to+\infty, gâˆ—â€‹(ctÂ±cdi)=gâˆ—â€‹(ct)Â±gâˆ—â€‹(cdi)superscriptğ‘”plus-or-minussubscriptğ‘ğ‘¡subscriptsuperscriptğ‘ğ‘–ğ‘‘plus-or-minussuperscriptğ‘”subscriptğ‘ğ‘¡superscriptğ‘”subscriptsuperscriptğ‘ğ‘–ğ‘‘g^{*}(c{t}\pm c^{i}{d})=g^{*}(c{t})\pm g^{*}(c^{i}{d}) N -> âˆ -> ğ‘N\to\infty, then.

Proposition P3. For any concept cdsubscriptğ‘ğ‘‘c{d},

Moving away from ctsubscriptğ‘ğ‘¡c{t} by +cdsubscriptğ‘ğ‘‘+c{d} or âˆ’cdsubscriptğ‘ğ‘‘-c{d} results in lesser degree of modification.

Proposition P4. If y0=gâˆ—â€‹(ca)subscriptğ‘¦0superscriptğ‘”subscriptğ‘ğ‘y{0}=g^{*}(c{a}) and.
lambda â€‹(ca)=0ğœ†subscriptğ‘ğ‘0\lambda(c{a})=0, then.

That is, if an anchor concept casubscriptğ‘ğ‘c{a} is used, and the guidance at casubscriptğ‘ğ‘c{a} is not affected, then the guidance vectors gâ€‹(ct)âˆ’gâ€‹(ca)ğ‘”subscriptğ‘ğ‘¡ğ‘”subscriptğ‘ğ‘g(c{t})-g(c{a}) and gâˆ—â€‹(ct)âˆ’gâˆ—â€‹(ca)superscriptğ‘”subscriptğ‘ğ‘¡superscriptğ‘”subscriptğ‘ğ‘g^{*}(c{t})-g^{*}(c{a}) are colinear.

3.2 Attacked inference framework.

Using the formalization of compositional property and inhibition objectives described in SectionÂ 3.1, we design the inputs that aim to circumvent concept inhibition. TableÂ 1 lists the inputs for gğ‘”g that result in the guidance vectors colinear with gâˆ—â€‹(ct)superscriptğ‘”subscriptğ‘ğ‘¡g^{*}(c{t}) or a sum containing it. We refer to the attacks that bypass concept inhibition via using arithmetics in concept space and compositional inference as ARC attacks.

Additionally, the attacks can be stacked to produce stronger guidance in the direction of ctsubscriptğ‘ğ‘¡c{t}. This is demonstrated for attacks A1 and A2 in Proposition P2, similar logic applies to stacking different attacks.

To preserve the control over the image generation, we combine the attack inputs with the original user-defined prompt. Thus, performing attack A1 to generate an image given prompt c1subscriptğ‘1c{1} means that instead of using Standard Inference to compute.

we use Compositional Inference to compute.

For example, for a target concept ctsubscriptğ‘ğ‘¡c{t}=â€œzebraâ€, prompt c1subscriptğ‘1c{1}=â€œzebra standing in the fieldâ€, if we implement A1 with cdsubscriptğ‘ğ‘‘c{d}=â€œcakeâ€ and ct+cdsubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘c{t}+c{d}=â€œcake in the shape of zebraâ€ the inference is.

FigureÂ 2 illustrates this example.

Our approach does not involve any optimization procedures. The setup required to perform the attacks consists exclusively of having access to the compositional inference of the model. If such access is given as a black-box API, no coding is required to perform the attacks. If the access is given as model weights, one can use existing implementations of compositional inference to perform the attacks (minimal coding might be required). The only computational overhead of our attacks consists of additional computations of gğ‘”g (forward pass of the U-Net) during inference for each additional concept used.

3.3 Attack implementations.

We test the proposed attacks on two of the types of inhibition considered in the literature: object categories and nudity. Table 2 lists the attacks considered in our experiments.

O1, O2, N1. Proposition P1 implies that for concepts cdsubscriptğ‘ğ‘‘c{d} sufficiently distant from ctsubscriptğ‘ğ‘¡c{t} such that linearity holds, the guidance gâ€‹(ct+cd)âˆ’gâ€‹(cd)ğ‘”subscriptğ‘ğ‘¡subscriptğ‘ğ‘‘ğ‘”subscriptğ‘ğ‘‘g(c{t}+c{d})-g(c{d}) approaches gâˆ—â€‹(ct)superscriptğ‘”subscriptğ‘ğ‘¡g^{*}(c{t}). We manually define concepts cdsubscriptğ‘ğ‘‘c{d} (â€˜cakeâ€™), and its combination ++ with another concept ctsubscriptğ‘ğ‘¡c{t} (ct+limit-fromsubscriptğ‘ğ‘¡c{t}+â€˜cakeâ€™=â€˜cake in the shape of ctsubscriptğ‘ğ‘¡c{t}â€™). We try to keep the combination ct+cdsubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘c{t}+c{d} as close to cdsubscriptğ‘ğ‘‘c{d} as possible to further minimize the degree of modification since closer to cdsubscriptğ‘ğ‘‘c{d} means further away from ctsubscriptğ‘ğ‘¡c{t}. This is why we use â€˜cake in the shape of ctsubscriptğ‘ğ‘¡c{t}â€™ instead of â€˜cake and ctsubscriptğ‘ğ‘¡c{t}â€™. This principle is used to design attacks O1 (cd=cakesubscriptğ‘ğ‘‘cakec{d}=\textsc{cake}) and N1 (cd=text, writtensubscriptğ‘ğ‘‘text, writtenc{d}=\textsc{text, written}). Here, concept cdsubscriptğ‘ğ‘‘c{d} can be viewed as a detour concept. Attack O2 extends O1 following the intuition provided by P2: stacking multiple signals produces stronger guidance in the direction of ctsubscriptğ‘ğ‘¡c{t}. O2 uses three detour concepts instead of one.

N2, N3.
In O1, we subtract the concept cdsubscriptğ‘ğ‘‘c{d}=â€˜cakeâ€™ in order to keep the generation of images unbiased with rescpect to the concept cdsubscriptğ‘ğ‘‘c{d}. Otherwise, the generated images would likely contain cdsubscriptğ‘ğ‘‘c{d} (FigureÂ 2, middle column) and in some cases cdsubscriptğ‘ğ‘‘c{d} can overpower ctsubscriptğ‘ğ‘¡c{t} (image contains a cake but no target concept). However, even though the inference that uses this guidance is biased towards cdsubscriptğ‘ğ‘‘c{d}, it still contains the guidance in the direction of the target concept and has lesser degree of modification (implication of P3). The subtraction of gâ€‹(cd)ğ‘”subscriptğ‘ğ‘‘g(c{d}) can be omitted, when biased generation can be considered a successful inhibition circumvention. In the case of ctsubscriptğ‘ğ‘¡c{t}=â€˜fruitsâ€™, N2 is equivalent to computing the guidance for a superset of concepts ct+cdsubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘c{t}+c{d}=â€˜fruits and vegetablesâ€™, and N3 is equivalent to computing the guidance for a subset of concepts ctâˆ’cdsubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘c{t}-c{d}=â€˜appleâ€™ in P4. Note, that cdsubscriptğ‘ğ‘‘c{d} is not explicitly defined in either of the attacks, we only define ctÂ±cdplus-or-minussubscriptğ‘ğ‘¡subscriptğ‘ğ‘‘c{t}\pm c{d}. These attacks can also be viewed as the reversed SLDÂ  approach with a general unsafe concept prompt (N2) or a more focused unsafe concept targeting a specific NudeNet category (N3).

O3. Attack O3 is based on P4 which implies that the guidance gâ€‹(ct)âˆ’gâ€‹(ca)ğ‘”subscriptğ‘ğ‘¡ğ‘”subscriptğ‘ğ‘g(c{t})-g(c{a}) remains colinear with gâˆ—â€‹(ct)âˆ’gâˆ—â€‹(ca)superscriptğ‘”subscriptğ‘ğ‘¡superscriptğ‘”subscriptğ‘ğ‘g^{*}(c{t})-g^{*}(c{a}) even after the inhibition. Running compositional inference with this guidance should maximize probability of target concept being present in the image and minimize the probability for the anchor concept. This prevents this attack from reproducing target and anchor concepts simultaneously (for example, zebra and horse in one image). This limitation is neglible if anchor is a concept similar to target, but becomes critical when the anchor concept is a superset of the target concept (for example, â€œrobotâ€ is superset of â€œR2D2â€). We recommend using a superset anchor concept for better resistance to this attack. Also note, that if ca=âˆ…subscriptğ‘ğ‘c{a}=\varnothing (empty string) is used in the inhibition, O3 reduces to +gâ€‹(ct)ğ‘”subscriptğ‘ğ‘¡+g(c{t}) since gâ€‹(âˆ…)=gâˆ—â€‹(âˆ…)=0ğ‘”superscriptğ‘”0g(\varnothing)=g^{*}(\varnothing)=0. We conclude this section by noting that manual selection of prompts is a common practice in the modern works in inhibition and semantic manipulation of diffusion models. Similar to how SLD does not optimize the safety prompt, or inhibition works do not optimize the target or anchor concept prompts, in this work we omit the analysis of optimal choice of cdsubscriptğ‘ğ‘‘c{d}. While different cdsubscriptğ‘ğ‘‘c{d} can yield different results, our primary goal is to demonstrate that such cdsubscriptğ‘ğ‘‘c{d} exist and can be used to reproduce the target concept. The fact that such detour concepts can be easily picked by hand is an advantage of our approach, making the attacks interpretable and extremely easy to implement by an adversary. Additionally, we focus on universally applicable attacks, such that the same attack (for example O1) would work for multiple concepts (for example zebra, golf ball, and so on). In practice, instead of using a generic detour concept (â€˜cakeâ€™, â€˜textâ€™), the attacker could come up with a target-specific detour concepts (for example â€œZebstrikaâ€-â€œPokemonâ€ for â€œzebraâ€) that might work better for a given target.

4. Experiments.

We quantitatively evaluate the proposed attack implementations on the models that were inhibited for nudity in SectionÂ 4.1; object categories and recognizable figures in SectionÂ 4.2. Qualitative results can be seen in FiguresÂ 1, 2, 5. We adopt Stable Diffusion 1.41.41.4 as the base model for our experiments, as this model is used by the inhibition works. In all the experiments, for each prompt, we generate images using 5 random seeds for each generation mode. Generation modes consist of the Standard Inference using the original SD model, Standard Inference using the inhibited model, and Compositional Inference for each of our attacks (O1-3 for objects, N1-3 for nudity). As described in SectionÂ 3.1, each attack defines the additional concepts used in the generation.

The generation parameters (noise schedule, guidance scale, and so on) are selected in accordance with each of the inhibition works. Since (UCE and ESD), (AC) and (SA) use different generation parameters, the baselines are also slightly different for the three groups.

4.1 Circumventing Nudity Inhibition.

First, we attack the inhibition of the â€œnudityâ€ concept.

Models. We use the model weights released by the authors of ESDÂ  and Selective AmnesiaÂ ; for UCEÂ , we inhibit the model for the prompt â€œnudityâ€ using the official implementation. We do not evaluate SLDÂ  since both our attacks and SLD require access to the compositional inference, which means SLD inhibition can be trivially disabled or negated in this scenario (achieving baseline level). We do not evaluate ACÂ  in this experiment since it has no delineated protocol for inhibiting nudity.

Measure.
A pre-trained NudeNetÂ  model is used to detect nudity in the generated images, and the number of images that contain a given nudity category is used as the final metric.

Prompts. We use I2P datasetÂ  â€” a collection of prompts that invoke nudity, violence or other inappropriate content in the generated images. In order to limit the experiments to nudity, we follow and filter a total of 959595 prompts that have nudity percentage value greater than 50%.

Results. We report the number of generated images that contain NudeNet categories in each generation mode for every inhibition method in FigureÂ 3. Our results show that while inhibtition significantly reduces the rate of nudity in the images generated using standard inference, the inhibition does not entirely eradicate the concept from the model. The modified models can still be used to generate images with undesired content for each of the three considered nudity categories. In some cases, the inhibited models can generate images with nudity even more reliably than the unmodified baseline model.

4.2 Circumventing Object Inhibition.

Next, we evaluate the attacks against the inhibition of object categories and recognizable characters. Detailed information can be found in the supplementary.

Concepts.
We extend ImagenetteÂ  set of categories (casstte player, chain saw, church, English springer spaniel, French horn, garbage truck, gas pump, golf ball, parachute, tench) used in the evaluation ofÂ  with additional ImageNet categories (academic gown, paper towel, and zebra), as well as R2-D2 and Snoopy characters used inÂ .

Models. We obtain the inhibited models by using the official code released by the authors of ACÂ , ESDÂ , and UCEÂ . Fine-tuning in AC and ESD-u is performed using the suggested learning rates and number of iterations, 100 and 1000 respectively. We additionally evaluate the AC model fine-tuned for 200 iterations in order to test the attacks against stronger inhibition.

Measure.
To quantitatively evaluate concept inhibition in a diffusion model, inhibition works propose to use the original and the inhibited models to generate images for a set of prompts and then measure how much of the target concept is â€œpresentâ€ in the two sets of images. A smaller presence value of the target concept in the generated images indicates better concept inhibition. The same methodology and the same metrics can be used to measure the efficiency of the attacks (although with flipped optimal direction).

Following ACÂ , we use CLIP ScoreÂ  to measure the presence of the target concept in the generated images. Given a distribution of CLIP Scores computed for a set of prompts, AC uses the mean of this distribution as the metric of concept reproduction in the generated images. Despite having the same mean, the sets of scores [0.5,0.5]0.50.5[0.5,0.5] and [0.1,0.9]0.10.9[0.1,0.9] can correspond to situations when the concept is present in neither images (but the images have some correlation, for example similar textures) or distinctly present in one of them. We propose a metric that considers a threshold on the whole distribution of the CLIP Scores as a more detailed measure of the concept presence in the generated images. We use baseline model scores percentiles as thresholds, to normalize for the differences in the CLIP Score values for different concepts in the original model.
Normalized Reproduction rate at percentilepğ‘p (NR@p) is computed as the percent of images such that CLIP Score between the image and target concept string is higher than the pğ‘p-th percentile of the baseline scores. By definition, NR rate for the baseline scores approaches 1âˆ’p1ğ‘1-p for every value of pğ‘p.

Prompts. FollowingÂ , we use Chat-GPT APIÂ  for prompt generation. We generate 202020 prompts for each concept.

Results. We report the target concept NR rates for regular and attacked generations using models modified with the three inhibition techniques: AC, ESD-u, and UCE, averaged over all concepts in FigureÂ 4. We observe that for every percentile, our attacks result in a significantly higher a higher concept reproduction rate, that is fraction of images with high CLIP Score values. This can be especially critical at high percentile values corresponding to the images that have target concept present in a more pronounced way.

We see, that our attacks significantly overcome the inhibition when a suggested default value of 100 iterations is used in AC (AC-100). When a stronger inhibition is used (AC-200), our attacks are still successful, but to a lesser extent. ESD-u and UCE have higher inhibition rates but our attacks still increase the reproduction rates manyfold, sometimes generating multiple images where 0 images were generated using the standard inference. It is worth noting, that higher inhibition of ESD-u and UCE, seemingly, comes at the cost of reduced quality and variation in the generated images for other concepts.

We demonstrate reproduction rates for an individual case of inhibiting â€œzebraâ€ with AC-100 with an anchor prompt â€œhorseâ€ and the attacks in FigureÂ 5. 5. Discussion.

A straightforward conclusion from the presented work is that the current methods for inhibiting concepts in Diffusion Models are not robust to compositional inference attacks, and inhibited models should still be guard-railed using post-hoc techniques in high-risk scenarios. In order to defend against compositional inference attacks, one has to break the hypothesis H1, that is, modify the outputs globally (for all concepts), rather than locally (in the vicinity of the target).

A more general, and more important, contribution consists of building a framework for understanding how linearity of conditional guidance can have an impact on image generation process. This understanding is crucial when developing safety mechanisms in diffusion models. For example, if instead of a concept inhibition, a watermakring method is developed such that its optimization task follows EquationÂ 3, and H1 holds (the changes in conditional guidance are local), then such watermarking method would be vulnerable to the compositional inference attacks. Our work opens up the floor for further investigation in this direction, and we believe more research is necessary on the concept space and linearity of conditional guidance to ensure safe and robust editing of diffusion models.
