Multi-Concept Customization of Text-to-Image Diffusion.

Abstract.

While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (\vbox∼6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple, new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms several baselines and concurrent works, regarding both qualitative and quantitative evaluations, while being memory and computationally efficient.

![Image](https://media.arxiv-vanity.com/render-output/7106309/x1.png)

1. Introduction.

Recently released text-to-image models  have represented a watershed year in image generation. By simply querying a text prompt, users are able to generate images of unprecedented quality. Such systems can generate a wide variety of objects, styles, and scenes – seemingly “anything and everything”.

However, despite the diverse, general capability of such models, users often wish to synthesize specific concepts from their own personal lives. For example, loved ones such as family, friends, pets, or personal objects and places, such as a new sofa or a recently visited garden, make for intriguing concepts. As these concepts are by nature personal, they are unseen during large-scale model training. Describing these concepts after the fact, through text, is unwieldy and unable to produce the personal concept with sufficient fidelity.

This motivates a need for model customization. Given the few user-provided images, can we augment existing text-to-image diffusion models with the new concept (for example, their pet dog or a “moongate” as shown in Figure 1)? The fine-tuned model should be able to generalize and compose them with existing concepts to generate new variations. This poses a few challenges – first, the model tends to forget  or change  the meanings of existing concepts: for example, the meaning of “moon” being lost when adding the “moongate” concept. Secondly, the model is prone to overfit the few training samples and reduce sampling variations.

Moreover, we study a more challenging problem, compositional fine-tuning – the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, for example, pet dog in front of moongate (Figure 1). Learning multiple new concepts poses additional challenges, such as concept mixing and concept omission.

In this work, we propose a fine-tuning technique, Custom Diffusion for text-to-image diffusion models. Our method is computationally and memory efficient. To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers . Fine-tuning these is sufficient to update the model with the new concept. To prevent model forgetting, we use a small set of real images with similar captions as the target images. We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results. To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.

We build our method on Stable Diffusion  and experiment on various datasets with as few as four training images. For adding single concepts, our method shows better text alignment and visual similarity to the target images than concurrent works and baselines. More importantly, our method can compose multiple new concepts efficiently, whereas concurrent methods struggle and often omit one. Finally, our method only requires storing a small subset of parameters (3% of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, 2−4× faster compared to concurrent works). Please find the code and data at our website.

2. Related Work.

Deep generative models Generative models aim to synthesize samples from a data distribution, given a set of training examples. These include GANs , VAEs , auto-regressive , flow-based , and diffusion models . To improve controllability, these models can be conditioned on a class , image , or text prompt . Our work mainly relates to text-conditioned synthesis . While earlier works  were limited to a few classes , recent text-to-image models , trained on extremely large-scale data, have demonstrated remarkable generalization ability. However, such models are by nature generalists and struggle to generate specific instances like personal toys or rare categories, for example, “moongate”. We aim to adapt these models to become specialists in new concepts.

Image and model editing. While generative models can sample random images, a user often wishes to edit a single, specific image. Several works aim at leveraging the capabilities of generative models, such as GANs  or diffusion models  towards editing. A challenge is representing the specific image in the pre-trained model, and such methods employ per-image or per-edit optimization. A closely related line of work edits a generative model directly . Whereas these methods aim to customize GANs, our focus is on text-to-image models.

Transfer learning.
A method of efficiently producing a whole distribution of images is leveraging a pretrained model and then using transfer learning . For example, one can adapt photorealistic faces into cartoons . To adapt with just a few training images, efficient training techniques  are often useful. Different from these works, which focus on tuning whole models to single domains, we wish to acquire multiple new concepts without catastrophic forgetting . By preserving the millions of concepts captured in large-scale pretraining, we can synthesize the new concepts in composition with these existing concepts. Relatedly, several methods  propose to train adapter modules for large-scale models in the discriminative setting. In contrast, we adapt a small number of existing parameters and do not require additional parameters.

Adapting text-to-image models. Similar to our goals, two concurrent works, DreamBooth  and Textual Inversion , adopt transfer learning to text-to-image diffusion models  via either fine-tuning all the parameters  or introducing and optimizing a word vector  for the new concept. Our work differs in several aspects. First, our work tackles a challenging setting: compositional fine-tuning of multiple concepts, where concurrent works struggle. Second, we only fine-tune a subset of cross-attention layer parameters, which significantly reduces the fine-tuning time. We find these design choices lead to better results, validated by automatic metrics and human preference studies.

3. Method.

Our proposed method for model fine-tuning, as shown in Figure 2, only updates a small subset of weights in the cross-attention layers of the model. In addition, we use a regularization set of real images to prevent overfitting on the few training samples of the target concepts. In this section, we explain our design choices and final algorithm in detail.

3.1 Single-Concept Fine-tuning.

Given a pretrained text-to-image diffusion model, we aim to embed a new concept in the model given as few as four images and the corresponding text description. The fine-tuned model should retain its prior knowledge, allowing for novel generations with the new concept, based on the text prompt. This can be challenging as the updated text-to-image mapping might easily overfit the few available images.

In our experiments, we use Stable Diffusion  as our backbone model, which is built on the Latent Diffusion Model (LDM) . LDM first encodes images into a latent representation, using hybrid objectives of VAE , PatchGAN , and LPIPS , such that running an encoder-decoder can recover an input image. They then train a diffusion model  on the latent representation with text condition injected in the model using cross-attention.

Learning objective of diffusion models.
Diffusion models  are a class of generative models that aim to approximate the original data distribution q(x0) with ptheta (x0):

where x1 to xT are latent variables of a forward Markov chain s.t. xt=√alpha tx0+√1−alpha tϵ. The model is trained to learn the reverse process of a fixed-length (usually 1000) Markov chain. Given noisy image xt at timestep t, the model learns to denoise the input image to obtain xt−1. The training objective of the diffusion model can be simplified to:

where ϵtheta is the model prediction and wt is a time-dependent weight on the loss. The model is conditioned on timestep t and can be further conditioned on any other modality c, for example, text. During inference, a random Gaussian image (or latent) xT is denoised for fixed timesteps using the model .

A naïve baseline for the goal of fine-tuning is to update all layers to minimize the loss in Eqn. 2 for the given text-image pairs. This can be computationally inefficient for large-scale models and can easily lead to overfitting when training on a few images. Therefore, we aim to identify a minimal set of weights that is sufficient for the task of fine-tuning.

Rate of change of weights. Following Liet al. , we analyze the change in parameters for each layer in the fine-tuned model on the target dataset with the loss in Eqn. 2, that is, Delta l=||theta ′l−theta l||/||theta l||,
where theta ′l and theta l are the updated and pretrained model parameters of layer l. These parameters come from three types of layers – (1) cross-attention (between the text and image), (2) self-attention (within the image itself), and (3) the rest of the parameters, including convolutional blocks and normalization layers in the diffusion model U-Net. Figure 3 shows the mean Delta l for the three categories when the model is fine-tuned on “moongate” images. We observe similar plots for other datasets. As we see, the cross-attention layer parameters have relatively higher Delta compared to the rest of the parameters. Moreover, cross-attention layers are only 5% of the total parameter count in the model. This suggests it plays a significant role during fine-tuning, and we leverage that in our method.

Model fine-tuning.
Cross-attention block modifies the latent features of the network according to the condition features, that is, text features in the case of text-to-image diffusion models. Given text features c in Rs×d and latent image features f in R(h×w)×l, a single-head cross-attention  operation consists of Q=Wqf,K=Wkc,V=Wvc.
, and a weighted sum over value features as:

where Wq, Wk, and Wv map the inputs to a query, key, and value feature, respectively, and d′ is the output dimension of key and query features. The latent feature is then updated with the attention block output. The task of fine-tuning aims at updating the mapping from given text to image distribution, and the text features are only input to Wk and Wv projection matrix in the cross-attention block. Therefore, we propose to only update Wk and Wv parameters of the diffusion model during the fine-tuning process. As shown in our experiments, this is sufficient to update the model with a new text-image paired concept. Figure 4 shows an instance of the cross-attention layer and the trainable parameters.

Text encoding.
Given target concept images, we require a text caption as well. If there exists a text description, for example, moongate, we use that as a text caption. For personalization-related use-case where the target concept is a unique instance of a general category, for example, pet dog, we introduce a new modifier token embedding, that is, V∗ dog. During training, V∗ is initialized with a rare occurring token embedding and optimized along with cross-attention parameters. An example text caption used during training is, photo of a V∗ dog.

Regularization dataset.
Fine-tuning on the target concept and text caption pair can lead to the issue of language drift . For example, training on “moongate” will lead to the model forgetting the association of “moon” and “gate” with their previously trained visual concepts, as shown in Figure 5. Similarly, training on a personalized concept of V∗ tortoise plushy can leak, causing all examples with plushy to produce the specific target images. To prevent this, we select a set of 200 regularization images from the LAION-400M  dataset with corresponding captions that have a high similarity with the target text prompt, above threshold 0.85 in CLIP  text encoder feature space.

3.2 Multiple-Concept Compositional Fine-tuning.

Joint training on multiple concepts.
For fine-tuning with multiple concepts, we combine the training datasets for each individual concept and train them jointly with our method. To denote the target concepts, we use different modifier tokens, V∗i, initialized with different rarely-occurring tokens and optimize them along with cross-attention key and value matrices for each layer. As shown in Figure 7, restricting the weight update to cross-attention key and value parameters leads to significantly better results for composing two concepts compared to methods like DreamBooth, which fine-tune all the weights.

Constrained optimization to merge concepts.
As our method only updates the key and value projection matrices corresponding to the text features, we can subsequently merge them to allow generation with multiple fine-tuned concepts. Let set {Wk0,l,Wv0,l}Ll=1 represent the key and value matrices for all L cross-attention layers in the pretrained model and {Wkn,l,Wvn,l}Ll=1 represent the corresponding updated matrices for added concept n in {1⋯N}. As our subsequent optimization applies to all layers and key-value matrices, we will omit superscripts {k,v} and layer l for notational clarity. We formulate the composition objective as the following constrained least squares problem:

Here, C in Rs×d is the text features of dimension d. These are compiled of s target words across all N concepts, with all captions for each concept flattened out and concatenated. Similarly, Creg in Rsreg×d consists of text features of \vbox∼1000 randomly sampled captions for regularization. Intuitively, the above formulation aims to update the matrices in the original model, such that the words in target captions in C are mapped consistently to the values obtained from fine-tuned concept matrices. The above objective can be solved in closed form, assuming Creg is non-degenerate and the solution exists, by using the method of Lagrange multipliers :

We show full derivation in the Appendix A. Compared to joint training, our optimization-based method is faster (\vbox∼2 seconds) if each individual fine-tuned model exists. Our proposed methods lead to the coherent generation of two new concepts in a single scene, as shown in Section 4.2. Training details.
We train the models with our method for 250 steps in single-concept and 500 steps in two-concept joint training, on a batch size of 8 and learning rate 8×10−5. During training, we also randomly resize only the target images from 0.4−1.4× and append the prompt “very small”, “far away” or “zoomed in”, “close up” accordingly to the text prompt based on resize ratio. We only backpropagate the loss on valid regions. This leads to faster convergence and improved results. We provide more training and architecture details in the Appendix D.

4. Experiments.

In this section, we show the results of our method on multiple datasets in both single concept fine-tuning and composition of two concepts on the Stable Diffusion model .

Datasets. We perform experiments on ten target datasets spanning a variety of categories and varying training samples. It consists of two scene categories, two pets, and six objects, as shown in Figure 8. We show that the pretrained model cannot generate these concepts even with long text descriptions in Figure 20 in the Appendix.

Evaluation metrics.
We evaluate our method on (1) Image-alignment, that is, the visual similarity of generated images with the target concept, using similarity in CLIP image feature space , (2) Text-alignment of the generated images with given prompts, using text-image similarity in CLIP feature space , and (3) KIDon a validation set of 500 real images of a similar concept retrieved from LAION-400M to measure overfitting on the target concept (for example, V∗ dog)and forgetting of existing related concepts (for example, dog). (4) We also perform a human preference study of our method with baselines. Unless mentioned otherwise, we use 200 steps of DDPM sampler with a scale 6. The prompts used for quantitative and human evaluation are shown on our website.

Baselines.
We compare our method with the two concurrent works, DreamBooth(third-party implementation ) and Textual Inversion. DreamBooth fine-tunes all the parameters in the diffusion model, keeping the text transformer frozen, and uses generated images as the regularization dataset. Each target concept is represented by a unique identifier, followed by its category name, that is, “[V] category”, where [V] is a rarely occurring token in the text token space and not optimized during fine-tuning. Textual Inversion optimizes a new V∗ token for each new concept. We also compare with the competitive baseline of Custom Diffusion (w/ fine-tune all), where we fine-tune all the parameters in the U-Net  diffusion model, along with the V∗ token embedding in our method. We provide implementation details for all baselines in the supplement.

4.1 Single-Concept Fine-tuning Results.

Qualitative evaluation.
We test each fine-tuned model on a set of challenging prompts. This includes generating the target concept in a new scene, in a known art style, composing it with another known object, and changing certain properties of the target concept: for example, color, shape, or expression. Figure 6 shows the sample generations with our method, DreamBooth, and Textual Inversion. Our method, Custom Diffusion, has higher text-image alignment while capturing the visual details of the target object. It performs better than Textual Inversion and is on par with DreamBooth while having a lower training time and model storage (\vbox∼5× faster and 75MB vs 3GB storage).

Quantitative evaluation.
We evaluate on 20 text prompts and 50 samples per prompt for each dataset, resulting in a total of 1000 generated images. We use DDPM sampling with 50 steps and a classifier-free guidance scale of 6 across all methods. As shown in Figure 8, our method outperforms the concurrent methods . Also, Custom Diffusion works on par with our proposed baseline of fine-tuning all the weights in the diffusion model, while being more computationally and time efficient. Table 1 also shows the KID of generated images by each fine-tuned model on a reference dataset, with captions similar to the fine-tuned concept. As we observe, our method has lower KID than most baselines, which suggests less overfitting to the target concept.

Computational requirements Training time of our method is \vbox∼6 minutes (2 A100 GPUs), compared to 20 minutes for Ours (w/ fine-tune all) (4 A100s), 20 minutes for Textual Inversion (2 A100s), and \vbox∼1 hour for DreamBooth (4 A100s). Also, since we update only 75MB of weights, our method has low memory requirements for storing each concept model. We keep the batch size fixed at 8 across all experiments. In Appendix B, we show that the updated matrices can be compressed to further reduce model storage.

4.2 Multiple-Concept Fine-tuning Results.

We show the results of generating two new concepts in the same scene on the following five pairs: (1) Moongate + Dog, (2) Cat + Chair, (3) Wooden Pot + Cat, (4) Wooden Pot + flower, and (5) Table + Chair. We compare our method with DreamBooth training on the two datasets together, using two different [V1] and [V2] tokens for each concept. For Textual Inversion, we perform inference using the individually fine-tuned tokens for each concept in the same sentence. We compare our method with the baselines on a set of 400 images generated with 8 prompts for each composition setting in Figure 8 and Table 1. We also compare with our baseline of sequentially training on the two concepts or fine-tuning all weights in our method. Our method performs better on all except the “Table+Chair” composition, where all methods perform comparably except Textual Inversion, which doesn’t perform well at composition as also shown in Figure 19 in the Appendix. This shows the importance of fine-tuning only the cross-attention parameters for composition. In the case of sequential training, we observe forgetting of the first concept. Figure 7 shows sample images of our proposed two methods and DreamBooth. As we can see, our method is able to generate the two objects in the same scene in a coherent manner while having high alignment with the input text prompt. We show more samples on our website.

4.3 Human Preference Study.

We perform the human preference study using Amazon Mechanical Turk. We do a paired test of our method with DreamBooth, Textual Inversion, and Ours (w/ fine-tune all). For text-alignment, we show the two generations from each method (ours vs. baseline) on the same prompt with the question – “Which image is more consistent with the text?”. In image-alignment, we show 2-4 training samples of the relevant concept along with the generated images (same as in text-alignment study) with the question – “Which image better represents the objects as shown in target images?”. We collect 800 responses for each questionnaire. As shown in Table 2, our method is preferred over baselines in both single-concept and multi-concept, even compared to Ours (w/ fine-tune all) method, which shows the importance of only updating cross-attention parameters.

4.4 Ablation and Applications.

In this section, we ablate various components of our method to show its contribution. We evaluate each experiment on the same setup as in Section 4.1. We show sample generations for ablation experiments on our website.

Generated images as regularization (Ours w/ Gen).
As detailed in Section 3.1, we retrieve similar category real images and captions to use as regularization during fine-tuning. Another way of creating the regularization dataset is to generate images from the pretrained model . We compare our method with this setup, that is, for the target concept of a “category” generate images using the prompt, photo of a {category}, and show results in Table 3. Using generated images results in a similar performance on the target concept. However, this shows signs of overfitting, as measured by KID on a validation set of similar category real images.

Without regularization dataset (Ours w/o Reg).
We show results when no regularization dataset is used. We train the model for half the number of iterations (the same number of target images seen during training). Table 3 shows that the model has slightly lower image-alignment and tends to forget existing concepts, as evident from high KID on the validation set.

Without data augmentation (Ours w/o Aug).
As mentioned in Section 3.2, we augment by randomly resizing the target images during training and append size-related prompts (for example, “very small”) to the text. Here, we show the effect of not using these augmentations. The model is trained for the same number of steps. Table 3 shows that no augmentation leads to lower visual similarity with target images.

Fine-tuning on a style.
We show in Figure 9 that our method can also be used for fine-tuning with specific styles. We change the text prompt for target images to A painting in the style of V∗ art . The regularization images are retrieved with captions having high similarity with “art”.

Image editing with fine-tuned models.
Similar to the pretrained model, our fine-tuned models can be used by existing image-editing methods. We show an example of using the Prompt-to-prompt  method in Figure 10. 5. Discussion and Limitations.

In conclusion, we have proposed a method for fine-tuning large-scale text-to-image diffusion models on new concepts, categories, personal objects, or artistic styles, using just a few image examples. Our computationally efficient method can generate novel variations of the fine-tuned concept in new contexts while preserving the visual similarity with the target images. Moreover, we only need to save a small subset of model weights. Furthermore, our method can coherently compose multiple new concepts in the same scene.

As shown in Figure 11, difficult compositions, for example, a pet dog and a pet cat, remain challenging. In this case, the pre-trained model also faces a similar difficulty, and our model inherits these limitations. Additionally, composing increasing three or more concepts together is also challenging. We show more analysis and visualization in the Appendix B.

Acknowledgment.
We are grateful to Nick Kolkin, David Bau, Sheng-Yu Wang, Gaurav Parmar, John Nack, and Sylvain Paris for their helpful comments and discussion, and to Allie Chang, Chen Wu, Sumith Kulal, Minguk Kang, and Taesung Park for proofreading the draft. We also thank Mia Tang and Aaron Hertzmann for sharing their artwork. This work was partly done by Nupur Kumari during the Adobe internship. The work is partly supported by Adobe Inc.
