Parts of Speech–Grounded⁠ Subspaces.
in Vision-Language Models.

Abstract.

Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased towards specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP’s joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (for example nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What’s more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (for example artists’ painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists’ styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification. Our code is available at: https://github.com/james-oldfield/PoS-subspaces.

eqlisti 2060.

1. Introduction.

Many recent advances in machine learning have been driven by vision-language (VL) models’ ability to learn powerful, generalisable image representations from natural language supervision. The image features from VL models well-capture representations of many visual attributes as evidenced by the broad applicability they have found for use in downstream tasks. The image or text encoders of CLIP in particular have been used for controllable image synthesis , image captioning , and multiple other discriminative tasks. However, modelling the many different visual modalities in a single vector representation is not without its drawbacks–recent work shows that CLIP’s visual representations are often entangled. For example, find that specific neurons fire in response to both images containing a visual concept and images of text relating to the same concept. This leaves CLIP open to vulnerabilities in the form of ‘Typographic attacks’–writing another class name as text on the image can often cause CLIP to predict this class with a higher probability than that of the original image’s true category. Other recent works show that CLIP’s visual representations encode task-specific attributes (such as of the object or action depicted in an image) in an unpredictable manner, and often the embedding is biased in the prominence with which it encodes different modalities. We show in Figure 0(a) a motivating example of this problem identified by –the ‘goldfish’ noun embedding dominates the image’s representation despite there being multiple additional labels which accurately describe important information about the image’s contents. As a visual example, we find that CLIP encodings of text prompts containing ‘visually polysemous’ phrases of artists’ names lead CLIP-based text-to-image models to synthesize an unpredictable combination of both images of the artists and of artworks in their signature styles (as shown in Figure 0(b)). Multiple visual associations of the text prompt, including both the appearance of the artist themselves and the style of their artwork, are entangled in the same CLIP embedding. For VL representations to make for useful image features, it’s vital that the particular modalities of interest are indeed well-represented in the embedding. One popular means to this end is fine-tuning the representations for specific downstream tasks. However, this not only requires additional computation but makes the restrictive assumption of the existence of labelled data for each task.

In this paper, we address the problem of better disentangling the modes of visual variation in CLIP’s shared vision-language space. In particular, we ask the question: do there exist subspaces in CLIP’s joint VL space that capture the representations of the ‘content’ of an image or text, that are invariant to its ‘appearance’? To take the first steps towards achieving this we leverage the association between parts of speech in natural language and specific modes of visual variation: our learnt noun subspace isolates representations of the ‘object’ of an image or text prompt (for example an animal in an image, or the noun described in a sentence), and the adjective space its appearance (for example whether an object is shiny, or a scene is snowy). This image-text semantic similarity is instilled in the representations through CLIP’s contrastive learning training objective, through which it is encouraged to learn image encodings that have large cosine similarity in the shared vision-language space with text encodings of captions describing its visual content. We show how by using example words in the parts of speech categories in the WordNet database, one can extract representations of both image and text embeddings that better isolate the individual modes of visual variation. This is in contrast to the method of VL model ‘prompting’ , which often steers the embeddings of either only the text or image modality. We take inspiration from the related line of work that finds so-called ‘interpretable directions’ in latent space that capture high-level semantic attributes–however, we learn subspaces that capture variation uniquely present amongst representations of words of particular parts of speech. We achieve this by formulating an appropriate objective function which we show can be manipulated into a well-known trace maximisation problem with a fast solution given in closed form. Since the CLIP representations live on the hypersphere , we further propose a manifold generalisation of the subspaces (illustrated in Figure 2) that share the property of capturing the variance of only the desired visual attributes, yet better respect the manifold on which the data lie. Concretely, we compute the proposed component analysis in the tangent space to the sphere’s intrinsic mean, which can be seen as a local approximation of the manifold.

The method’s ability to disentangle visual modes of variation is measured both qualitatively and quantitatively. Using a popular CLIP-based text-to-image model we demonstrate visually how the learnt PoS subspaces can better separate the content from the style associated with a text prompt. We show, for example, how simply projecting onto the orthogonal complements of the noun and adjective subspaces respectively can more reliably produce images of either the artists’ work, or of the artists themselves, as shown in rows 2 and 3 of Figure 0(b). We find removing a CLIP representations’ adjective space component to be remarkably effective for preventing the visual imitation of artists’ styles in this new class of CLIP-based text-to-image models , addressing societal concerns of the technology. Further, we show our objective additionally facilitates learning subspaces corresponding to more specific visual appearances (for example ‘gory’). Projections onto the orthogonal complements of such subspaces consequently remove entire visual themes from text-to-image models, whilst preserving existing concepts through the PoS guidance.

Finally, we validate the subspaces’ ability to isolate visual modes of variation quantitatively through a measure of class invariance (comparing to two existing baselines), and by showing how the baseline zero-shot classification protocol in the learnt subspaces leads to higher accuracies on the ViT-B-32 CLIP model on 14/15 datasets considered. Our contributions can be summarised as follows:

1. We present a method for learning geometry-aware subspaces in CLIP’s shared vision-language space that disentangle representations of the content in an image or text prompt from the way it looks, using parts of speech as supervision. To the best of our knowledge, we are the first to use the semantic relationship between parts of speech and specific visual modes of variation to disentangle CLIP’s shared vision-language space.
2. We formulate and solve an appropriate trace maximisation problem that admits a fast closed-form solution respecting the manifold on which the data lie.
3. We validate our method’s success at disentangling the visual modes of variation both quantitatively and qualitatively: by visually separating a text prompt’s ‘content’ from its ‘appearance’ with text-to-image models for the former and through improved zero-shot classification in the submanifolds for the latter.
4. We show the model’s ability to further learn subspaces of more specific appearance-based variation (for example artists’ styles), providing a way of erasing entire visual themes from CLIP-based text-to-image models.

2. Method.

We first recall the basics of CLIP. We then motivate and introduce our objective function for learning parts of speech–specific linear subspaces in Section 2.1, and detail its closed-form solution in Section 2.1.1. Finally, in Section 2.2 we show how to learn subspaces of the tangent space to the CLIP VL hypersphere’s intrinsic mean, which better respects the geometry of the manifold on which the CLIP representations lie.

CLIP preliminaries.

Pre-trained image and text encoders map images and text respectively to latent representations zI,zT in Rd in a shared ‘vision-language’ (VL) space. For test-time zero-shot classification, candidate text prompts are first encoded with the text encoder. Then, the cosine similarity between an encoded image representation of interest and the text candidates is computed with S(zT,zI)=zT⊤zI/(||zT||⋅||zI||), after which the softmax function is applied to determine the most likely text label for the image. We now address the task of extracting representations of solely the target modes of variation in the section that follows.

2.1 Objective.

We seek a lower-dimensional subspace on which to project either a text or image CLIP representation z in Rd to predictably isolate the desired visual mode of variation. We achieve this through natural language supervision in the form of words from the different parts of speech. Let the elements of a set C index into the relevant word class of interest (that is C={N,A,V,R} for nouns, adjectives, verbs, and adverbs respectively), and Xi in Rd×n,forall i in C contain in their columns the CLIP encodings of n words belonging to word class i. Then, for a word class i in C of interest, we seek a k-dimensional subspace of Rd spanned by the columns of a learnt Wi in Rd×k.
in which the CLIP representations of the words in the class of interest i have a large norm and the remaining categories’ representations in C∖{i} are close to the zero vector. Intuitively, a hyperplane with this property models factors of variation that are uniquely present in representations of text that belong to a particular part of speech. We quantify this by formulating the following objective function:

where lambda in [0,1] is a hyperparameter that controls the importance of killing the variation in the non-target categories relative to preserving the variation in the target class.

2.1.1 Closed-form solution.

Imposing column orthonormality on Wi not only ensures its columns span a full k-dimensional subspace, but also allows Equation 1 to be solved in closed form. Concretely, we first manipulate the objective as follows.

where Ci=((1−lambda )XiX⊤i−∑j in C∖{i}lambda XjX⊤j). Having now reformulated our original objective in Equation 1 as a (constrained) trace maximisation problem, its solution Wi is given in closed form ([1] Corollary 4.3.39 of. Note that all summands in Ci are symmetric.) as the leading k eigenvectors of Ci.

For ease of presentation, we have assumed the number of data points in each class to be equal. One can account for class imbalance straightforwardly whilst retaining a solution in closed form however: multiplying each Frobenius norm term in the original objective of Equation 1 by 1np (where np is the number of columns of Xp,forall p in C) leads to Ci=(1−lambda niXiX⊤i−∑j in C∖{i}lambda njXjX⊤j) in Equation 4. Special cases.

To contrast the proposed objective with related decompositions, it’s instructive to consider the resulting subspaces for extreme values of lambda (for zero-mean data). In the limit case of lambda :=0 for example, Wi is given by the top principal components of the data points for one particular class i. Conversely, a value of lambda :=1 gives the bottom principal components of datapoints in all other classes j in C∖{i}. Thus, lambda can be seen as providing a trade-off between hyperplanes well-capturing the variance of the attributes in the target class and lying near-orthogonal to the points of the remaining classes. Finally, one recovers regular PCA when Ci:=∑j in C1njXjX⊤j.

2.2 From subspaces to submanifolds.

Through CLIP’s choice of the cosine similarity as an objective function during training, the unit-norm VL representations live on the hypersphere zI,zT in Sd−1 subset Rd. However, subspace learning in the ambient Euclidean space Rd does not respect the underlying geometry of the manifold on which the data lie. An orthogonal projection of a vector onto the learnt Euclidean subspaces is not guaranteed to result in a vector that remains on the sphere, even if all the input data do. Motivated by this, we extend the component analysis for linear subspaces developed in Equation 1 to geodesic submanifolds. As demonstrated in for PCA, an analogous approximate projection onto the geodesic submanifolds capturing the desired variances can be made by applying the exact same component analysis of Equation 1 in the tangent space.
Tmu Sd−1 subset Rd.
to the VL sphere data’s intrinsic mean mu in Sd−1 instead. To this end, we use the so-called Logarithmic Map Logp:Sd−1 -> TpSd−1, which maps points on the sphere to the tangent space at a reference point p in Sd−1 and its inverse, the Exponential Map Expp:TpSd−1 -> Sd−1 to map points back onto the hypersphere (whose well-known definitions are provided in the supplementary material).

We can then compute the subspace of the tangent space to the intrinsic mean spanned by the columns of a ^Wi in Rd×k for word class i as the leading k eigenvectors of.
^Ci=∑n(1−lambda )Logmu (xin)Logmu (xin)⊤−∑j in C∖{i}lambda Logmu (xjn)Logmu (xjn)⊤. We have again assumed an equal number of class data points purely for ease of presentation. This whole process is visualised in Figure 2. By projecting onto the learnt subspaces of Tmu Sd−1, one can better isolate or remove the visual attributes associated with part of speech i. To isolate or kill the attributes we compute the projection onto the column space (i) or orthogonal complements (ii) respectively with.

Projection onto the Euclidean subspaces can be computed using the projection matrices Pi=WiW⊤i and P⊥i=Id−Pi. The way in which one can extract representations relating to the visual modes of variation for the parts of speech is explored in detail in Section 3. 3. Experiments.

Here we present both qualitative (Section 3.1) and quantitative (Section 3.2) experiments to validate the model’s ability to disentangle the object in a CLIP text or image representation from its appearance. We henceforth use ‘subspace’ to refer throughout to the manifold generalisation from Section 2.2. Implementation details.

For all experiments, we use the following 4 parts of speech: nouns, adjectives, verbs, and adverbs. Our labelled data points (with which we compute the closed-form solution of Equation 4) for these parts of speech are given by the WordNet database. There are a total of 112219, 18021, 7295, and 3910 text-string data points from each of the categories respectively, after filtering out any word that appears in two or more parts of speech. We set lambda :=1/2 for all experiments. For all quantitative results, we use the base CLIP ViT-B-32 model. Please see the supplementary material for ablation studies on lambda and experiments on additional CLIP architectures.

3.1 Qualitative results.

3.1.1 Visual disentanglement.

Recent CLIP-based text-to-image models (TTIMs) learn a mapping from the CLIP embeddings to synthetic images depicting visually a text prompt –a process that show is prone to also inheriting task bias. We begin in this section by following the experimental protocol of , using a recent popular CLIP-based TTIM from LAION to demonstrate visually how our PoS subspaces can succeed in predictably isolating visual variation in the CLIP representations pertaining to either the object described in a text prompt or its appearance.

As one motivating example, we first study a curious instance of ‘visually polysemous’ natural language phrases–terms that have multiple visual associations. In particular, we find that when prompted with artists’ names, TTIMs produce both artworks in their style and images of the person themselves (for example the top row of Figure 3). That is to say, CLIP entangles in its representation these two dual meanings of the artists–their works’ style and their physical appearance. We find that the PoS subspaces offer an intuitive way of reliably separating factors of visual variation in the representations for not just visually polysemous phrases, but also for natural language prompts more generally.

Concretely, projecting onto the orthogonal complement of the noun subspace Pi ⊥N(zT) successfully removes from the embedding visual information about the object/content described implicitly in a text prompt T. For example, as visualised in row 2 of Figure 3 by reliably synthesising just artwork in an artist’s style, or by leaving just the snowy or multicoloured appearance-based textures. On the other hand, projecting onto the adjective subspace’s orthogonal complement Pi ⊥A(zT) removes the visual appearances and styles associated with a text description (row 3 of Figure 3). For the visually polysemous artists’ names, this isolates the representations of the artists themselves as humans–removing representations of their artwork styles. For the more complex prompts, this produces images of just the basic object described in the text prompt instead, such as the penguin or New York City. Many more examples of this visual disentanglement, details on the experimental setup, and ablation studies can be found in the supplementary material.

3.1.2 Style-blocking subspace projections.

One societal concern with free-form TTIMs is their ability to produce imitation artworks copying the style of artists. Here, we show how the learnt adjective subspace can be used as is as a step towards mitigating this. To achieve this, one simply modifies the TTIM forward pass to first project the CLIP text representations onto the orthogonal complement of the adjective subspace with Pi ⊥A(zT) before feeding it into the image generator. We see from the results in Figure 4 that this modification indeed prevents the imitation of the visual styles of a range of artists (even with multiple forms of sentence structure in the prompt), whilst still enabling a diverse set of images to be generated nonetheless.

Visual theme subspaces.

Whilst successfully preventing the visual imitation of many famous artists, applying the adjective subspace projection to every text prompt’s CLIP representation can restrict the ability to use adjectives to specify visual appearance. We find a further effective strategy is to build additional subspaces for more specific visual appearances (for example artists’ painting styles). Concretely, we embed 830 artists’ names and surnames in a new matrix Xi′ and solve Equation 1 using all PoS classes in the negative summation to prevent the destruction of existing concepts. In contrast to the adjective space, projection onto the orthogonal complement of this ‘artist subspace’ preserves adjective-based visual descriptions whilst also successfully preventing style imitation (Figure 4(a)). Crucially, we highlight that for the example shown in Figure 4(a), the artist’s name Qi Baishi is not present in the ‘training’ list of example artists, suggesting the subspace has learnt a more general notion of an artist rather than simply the variation for only those artists whose names are provided as supervision.

We suggest more generally that subspaces learnt from a collection of words describing specific themes (whilst retaining variation in the PoS data through the main objective) could be useful for erasing entire visual concepts (such as NSFW imagery) from the CLIP representations. An additional example of another such custom subspace is shown in Figure 4(b) for removing only gory/bloody visual appearances. Not only does this offer a way to erase specific appearances in CLIP-based text-to-image models, but might also offer application in discriminative tasks, such as being able to block CLIP-based retrieval of images of sensitive nature. Please see the supplementary material for additional details, results, and visualisation of failure cases.

3.2 Quantitative results.

Here we show quantitatively how the PoS–grounded subspaces can isolate the variation in the CLIP representation of either an image or text prompt. We validate this in two ways: through a class invariance metric in Section 3.2.1 and through zero-shot classification in Section 3.2.2. 3.2.1 Class invariance.

We first aim to quantify the extent to which our method results in subspaces that capture variation solely in the desired part of speech. We quantify this by a class invariance metric to measure how much each class’ data points are retained in the subspaces. Concretely, we compute the Frobenius norm 1nj||^W⊤iYj||2F where each column yjn=Logmu (xjn) contains the CLIP embeddings in the columns of Xj mapped to the tangent space, and nj is the number of data points in class j.

We compare the proposed method to two other component analyses performed in the tangent spaces. In Figure 5(a) we project onto a subspace learnt from only the data of the specific PoS of interest (for example the first row’s subspace learnt through principal geodesic analysis –maximising the variance for just the noun PoS data points), whilst in Figure 5(c) the subspace is learnt by simply minimizing the variance in the projections for the nuisance PoS classes (for example the first row’s subspace minimizing the variance for the remaining adjective, verb, and adverbs’ data points). We show the results of the proposed method with lambda =0.5 in Figure 5(b) for all combinations of parts of speech.

This quantity should be as close as possible to 0 in each of Figure 6’s subfigures’ off-diagonals i≠j and as close as possible to 1 along the figures’ diagonals i=j (if the subspaces capture visual variation that is unique to a particular word class). The learnt subspaces in Figure 5(b) evidently exhibit this pattern of disentanglement better than the baselines; the vectors have a large norm in their labelled class and a much smaller norm in the subspaces of the other categories.

3.2.2 Zero-shot subspace classification.

In this subsection, our goal is to further validate quantitatively how well our noun subspace can isolate the representations relating to the content in an image by studying the task of zero-shot image classification ([2] For example, the colour in which a digit is drawn is not relevant information for the task of classifying which number it is.), given this is a very common application of CLIP in practice. Importantly, these experiments serve as an additional way of measuring the subspaces’ ability to isolate task-relevant modes of variation in the CLIP representations. With this in mind, we consider the baseline zero-shot setting which computes the cosine similarity between all images in the training sets’ and names of the class labels’ CLIP representations. In our case, rather than computing the cosine similarity of the CLIP representations with S(zI,zT) we measure instead the similarity in the noun subspace with S(Pi N(zI),zT). We hypothesise that if the noun subspace indeed better isolates the object of an image–invariant to the particular style of the image–we should expect to see improvement to the baseline image classification.

We show the Top-1 accuracies for a large variety of datasets in Table 1. We find the noun submanifold projection to lead to improved zero-shot classification on 14/15 of the datasets considered with CLIP ViT-B-32. This is without needing any prompt engineering or domain knowledge about each dataset separately, confirming that the subspaces serve the intended role of being able to isolate the visual modalities of interest automatically. For all results in this subsection, we project onto a relatively large k:=500 dimensional subspace for PCA, PGA, and the proposed method. Please see the supplementary material for full results on 2 more alternative CLIP architectures, and an ablation study showing the benefit of using the geometry-aware submanifold over the Euclidean subspaces.

4. Related work.

Vision-language representations.

There has been much interest in studying the properties of the VL representations in large-scale models such as CLIP. In particular, show that there exist neurons that fire in response to both visual representations of a concept as well as to text relating to the concept. Viewing this as a form of entanglement, address this by learning a linear subspace in which written text is disentangled from its visual component. Whilst we also wish to disentangle visual concepts, we do not focus on disentangling written words from visual representations, but rather visual modalities more generally. Further, our solution is instead given in closed form and has far fewer hyperparameters to tune. Another popular approach to learning more useful VL representations is fine-tuning; use fine-tuning in combination with weight interpolation to improve results on specific tasks without affecting existing performance, whilst many other works fine-tune CLIP for specific domains such as action recognition , video recognition , image captioning , and more. Despite the success of fine-tuning, this comes with the restrictive requirement of task-specific labels. The method of ‘prompting’ is one alternative approach. At a high level, this technique learns a set of parameters (either in the form of ‘words’ in a prompt or as pixels appended to an input image ) to steer the input to produce more useful representations for specific downstream tasks. In contrast, our method disentangles the modes of variation directly in the joint vision-language representation space–this facilitates the ability to separate semantic information in both image and text representations. Consequently, the proposed method has direct application for both discriminative tasks and CLIP text-driven generative tasks.

Component analysis.

Let the matrices X1,X2 in Rd×n contain n data points of two classes in their columns. The Principal Component Analysis (PCA) computes the low-dimensional subspace of maximum variance as the leading eigenvectors of the empirical covariance matrix of all data points. One pertinent drawback of PCA is its unsupervised nature in disregarding the labels of the data points. Whilst one can learn class-specific subspaces via PCA on the relevant subset of data (that is the eigenvectors of zero-mean 1nX1X⊤1), there is nothing to prevent the remaining ‘nuisance’ class(es) from also having large variance in this learnt subspace. One method that provides a way to jointly maximise the variance in one class’ projected embeddings whilst minimizing that of another class is the Fukunaga-Koontz transform (FKT). FKT learns a class 1–specific subspace spanned by the columns of W1 in Rd×k as the leading eigenvectors of.
(UD−12)⊤X1X⊤1(UD−12), where U,D are the eigenvectors (stacked column-wise) and eigenvalues (along the diagonal) respectively of the matrix (X1X⊤1+X2X⊤2). Whilst FKT also learns a subspace in which solely the target class’ points have a large coefficient, the FKT operates on just two classes of interest and is more computationally expensive than the proposed method in requiring two eigendecompositions (rather than one). Another related supervised decomposition is the Fisher Discriminant Analysis (FDA) which learns a subspace in which classes can be easily discriminated. Despite FDA’s objective involving a similar trace maximisation form to Equation 4, the goals of FDA and the proposed objective are very different. FDA aims to minimise intra-class variation, whilst the proposed objective maximises the norm of vectors of the target class in the learnt subspaces, whilst minimising those of the remaining classes. Additionally, the proposed objective does not suffer from the same restrictively small upper bound on the dimensionality of the learnt subspace as FDA does through the low-rank of the between-scatter matrix. A comparison to the subspaces learnt with FDA, FKT, and the proposed objective is shown in the supplementary material to illustrate the differences visually.

5. Conclusion.

In this paper, we have proposed a method for learning geometry-aware subspaces in CLIP’s vision-language space that disentangle the modes of visual variation in representations of both images and text. To achieve this, we used the semantic relationship between parts of speech in natural language and specific visual modes of variation. We demonstrated the disentanglement qualitatively with a text-to-image model, showcasing the model’s ability to remove visual imitation of artists’ styles from synthetic images. Class invariance metrics and zero-shot classification results further validated the disentanglement quantitatively.

Limitations.

A common drawback of subspace learning approaches is choosing a good number of dimensions k. Our method inherits this limitation, and one must choose the appropriate value for the specific task. Despite this, the closed-form eigensolution means only a single fast computation is needed, and any desired number of eigenvectors can be used at test-time. Whilst the model has wide application for both generative and discriminative tasks, it is not able to perfectly separate the modes of variation for every possible image and text prompt.

6. Acknowledgements.

This research was supported by the EU’s Horizon 2020 programme H2020-951911 AI4Media project, and by a grant from The Cyprus Institute on Cyclone.
