StyleDrop: Text-to-Image Generation in Any Style.

Abstract.

Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce StyleDrop, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than 1% of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a single image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse  convincingly outperforms other methods, including DreamBooth  and textual inversion  on Imagen  or Stable Diffusion . More results are available at our project website: https://styledrop.github.io.

1. Introduction.

Text-to-image models trained on large image and text pairs have enabled the creation of rich and diverse images encompassing many genres and themes. The resulting creations have become a sensation, with Midjourney  reportedly being the largest Discord server in the world. The styles of famous artists, such as Vincent Van Gogh, might be captured due to the presence of their work in the training data. Moreover, popular styles such as “anime” or “steampunk”, when added to the input text prompt, may translate to specific visual outputs based on the training data. While many efforts have been put into “prompt engineering", a wide range of styles are simply hard to describe in text form, due to the nuances of color schemes, illumination and other characteristics. As an example, Van Gogh has paintings in different styles (for example, Figure 1, top row, rightmost three columns). Thus, a text prompt that simply says “Van Gogh” may either result in one specific style (selected at random), or in an unpredictable mix of several styles. Neither of these is a desirable outcome.

In this paper, we introduce StyleDrop ([1] “StyleDrop” is inspired by eyedropper (a.k.a color picker), which allows users to quickly pick colors from various sources. Likewise, StyleDrop lets users quickly and painlessly ‘pick’ styles from a single (or very few) reference image(s), building a text-to-image model for generating images in that style.) which allows significantly higher level of stylized text-to-image synthesis, using as few as one image as an example of a given style. Our experiments (Figure 1) show that StyleDrop achieves unprecedented accuracy and fidelity in stylized image synthesis. StyleDrop is built on a few crucial components: (1) a transformer-based text-to-image generation model ; (2) adapter tuning ; and (3) iterative training with feedback. For the first component, we find that Muse , a transformer modeling a discrete visual token sequence, shows an advantage over diffusion models such as Imagen  and Stable Diffusion  for learning fine-grained styles from single images. For the second component, we employ adapter tuning  to style-tune a large text-to-image transformer efficiently. Specifically, we construct a text input of a style reference image by composing content and style text descriptors to promote content-style disentanglement, which is crucial for compositional image synthesis . Finally, for the third component, we propose an iterative training framework, which trains a new adapter on images sampled from a previously trained adapter. We find that, when trained on a small set of high-quality synthesized images, iterative training effectively alleviates overfitting, a prevalent issue for fine-tuning a text-to-image model on a very few (for example, one) images. We study high-quality sample selection methods using CLIP score (for example, image-text alignment) and human feedback in Section 4.4.3, verifying the complementary benefit.

In addition to handling various styles, we extend our approach to customize not only style but also content (for example, the identifying/distinctive features of a given object or subject), leveraging DreamBooth. We propose a novel approach that samples an image of my content in my style from two adapters trained for content and style independently. This compositional approach voids the need to jointly optimize on both content and style images  and is therefore very flexible. We show in Figure 5 that this approach produces compelling results that combines personalized generation respecting both object identity and object style.

We test StyleDrop on Muse on a diverse set of style reference images, as shown in Figure 1. We compare with other recent methods including DreamBooth  and Textual Inversion , using Imagen  and Stable Diffusion  as pre-trained text-to-image backbones. An extensive evaluation based on prompt and style fidelity metrics using CLIP  and a user study shows the superiority of StyleDrop to other methods. Please visit our website and Appendix for more results.

2. Related Work.

Personalized Text-to-Image Synthesis has been studied to edit images of personal assets by leveraging the power of pre-trained text-to-image models. Textual inversion   and Hard prompt made easy (PEZ)  find text representations (for example, embedding, token) corresponding to a set of images of an object without changing parameters of the text-to-image model.

DreamBooth  fine-tunes an entire text-to-image model on a few images describing the subject of interest. As such, it is more expressive and captures the subject with greater details. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA  or adapter tuning , are adopted to improve its efficiency . Custom diffusion  and SVDiff  have extended DreamBooth to synthesize multiple subjects simultaneously. Unlike these methods built on text-to-image diffusion models, we build StyleDrop on Muse , a generative vision transformer. have shown learning styles with text-to-image diffusion models, but from a handful or a dozen of style reference images, and are limited to painting styles. We demonstrate on a wide variety of visual styles, including 3d rendering, design illustration, and sculpture, using a single style reference image.

Neural Style Transfer (NST). A large body of work  has investigated style transfer using deep networks by solving a composite objective of style and content consistency . Recently, has shown that quantizing the latent space leads to improved visual and style fidelity of NST compared to continuous latent spaces. While both output stylized images, StyleDrop is different from NST in many ways; ours is based on text-to-image models to generate content, whereas NST uses an image to guide content (for example, spatial structure) for synthesis; we use adapters to capture fine-grained visual style properties; we incorporate feedback signals to refine the style from a single input image.

Parameter Efficient Fine Tuning (PEFT) is a new paradigm for fine-tuning of deep learning models by only tuning a much smaller number of parameters, instead of the entire model. These parameters are either subsets of the original trained model, or small number of parameters that are added for the fine-tuning stage. PEFT has been introduced in the context of large language models , and then applied to text-to-image diffusion models  with LoRA  or adapter tuning . Fine-tuning of autoregressive (AR)  and non-autoregressive (NAR)  generative vision transformers has been studied recently , but without the text modality.

3. StyleDrop: Style Tuning for Text-to-Image Synthesis.

StyleDrop is built on Muse , reviewed in Section 3.1. There are two key parts. The parameter-efficient fine-tuning of a generative vision transformer (Section 3.2) and an iterative training with feedback (Section 3.3). Finally, we discuss how to synthesize images from two fine-tuned models in Section 3.4. 3.1 Preliminary: Muse , a masked Transformer for Text-to-Image Synthesis.

Muse  is a state-of-the-art text-to-image synthesis model based on the masked generative image transformer, or MaskGIT . It contains two synthesis modules for base image generation (256×256) and super-resolution (512×512 or 1024×1024). Each module is composed of a text encoder T, a transformer G, a sampler S, an image encoder E, and decoder D.
T maps a text prompt t in T to a continuous embedding space E. G processes a text embedding e in E to generate logits l in L for the visual token sequence. S draws a sequence of visual tokens v in V from logits via iterative decoding , which runs a few steps of transformer inference conditioned on the text embeddings e and visual tokens decoded from previous steps. Finally, D maps the sequence of discrete tokens to pixel space I. ([2] We omit the description of the super-resolution module for concise presentation, and point readers to for a full description of the Muse model.) To summarize, given a text prompt t, an image I is synthesized as follows:

where n in T is a negative prompt, lambda is a guidance scale, k is the synthesis step, and lk’s are logits, from which the next set of visual tokens vk+1’s are sampled. We refer to for details on the iterative decoding process. The T5-XXL encoder for T and VQGAN  for E and D are used. G is trained on a large amount of (image, text) pairs D using masked visual token modeling loss :

where M is a masking operator that applies masks to the tokens in vi. CEm is a weighted cross-entropy calculated by summing only over the unmasked tokens.

3.2 Parameter-Efficient Fine-Tuning of Text-to-Image Generative Vision Transformers.

Now we present a unified framework for parameter-efficient fine-tuning of generative vision transformers. The proposed framework is not limited to a specific model and application, and is easily applied to the fine-tuning of text-to-image (for example, Muse , Paella , Parti , RQ-Transformer ) and text-to-video (for example, Phenaki , CogVideo ) transformers, with a variety of PEFT methods, such as prompt tuning , LoRA , or adapter tuning , as in. Nonetheless, we focus on Muse , an NAR text-to-image transformer, using adapter tuning .

Following , we are interested in adapting a transformer G, while the rest (E, D, T) remain fixed. Let ˆG:V×E×Theta -> L a modified version of a transformer G that takes learnable parameters theta in Theta as an additional input. Here, theta would represent parameters for learnable soft prompts of prompt tuning or weights of adapter tuning. Figure 2 provides an intuitive description of ˆG with adapter tuning.

Fine-tuning of the transformer ˆG involves learning of newly introduced parameters theta , while existing parameters of G (for example, parameters of self-attention and cross-attention layers) remain fixed, with the learning objective as follows:

where Dtr contains a few (image, text) pairs for fine-tuning. Unlike DreamBooth  where the same text prompt is used to represent a set of training images, we use different text prompts for each input image to better disentangle content and style. Once trained, similarly to the procedure in Equation 2, we synthesize images from the generation distribution of ˆG(⋅,⋅,theta ). Specifically, at each decoding step k, we generate logits lk as follows:

where lambda A controls the level of adaptation to the target distribution by contrasting the two generation distributions, one that is fine-tuned ˆG(vk,T(t),theta ) and another that is not G(vk,T(t)), and lambda B controls the textual alignment by contrasting the positive (t) and negative (n) text prompts.

3.2.1 Constructing Text Prompts.

To train theta , we require training data Dtr={(Ii,ti)}Ni=1 composed of (image, text) pairs for style reference. In many scenarios, we may be given only images as a style reference. In such cases, we need to manually append text prompts.

We propose a simple, templated approach to construct text prompts, consisting of the description of a content (for example, object, scene) followed by the phrase describing the style. For example, we use a “cat” to describe an object in Table 1 and append “watercolor painting” as a style descriptor. Incorporating descriptions of both content and style in the text prompt is critical, as it helps to disentangle the content from style and let learned parameters theta model the style, which is our primary goal. While we find that using a rare token identifier  in place of a style descriptor (for example, “watercolor painting”) works as well, having such a descriptive style descriptor provides an extra flexibility of style property editing, which will be shown in Section 4.4.2 and Figure 7. 3.3 Iterative Training with Feedback.

While our framework is generic and works well even on small training sets, the generation quality of the style-tuned model from a single image can sometimes be sub-optimal. The text construction method in Section 3.2.1 helps the quality, but we still find that overfitting to content is a concern. As in red boxes of Figure 3 where the same house is rendered in the background, it is hard to perfectly avoid the content leakage. However, we see that many of the rendered images successfully disentangle style from content, as shown in the blue boxes of Figure 3. For such a scenario, we leverage this finding of high precision when successful and introduce an iterative training (IT) of StyleDrop using synthesized images by StyleDrop trained at an earlier stage to improve the recall (more disentanglement). We opt for a simple solution: construct a new training set with a few dozen successful (image, text) pairs (for example, images in blue box of Figure 3) while using the same objective in Equation 3. IT results in an immediate improvement with a reduced content leakage, as in Figure 3 green box. The key question is how to assess the quality of synthesized images.

CLIP scoremeasures the image-text alignment. As such, it could be used to assess the quality of generated images by measuring the CLIP score (that is, cosine similarity of visual and textual CLIP embeddings). We select images with the highest CLIP scores and we call this method an iterative training with CLIP feedback (CF). In our experiments, we find that the CLIP score to assess the quality of synthesized images is an efficient way of improving the recall (that is, textual fidelity) without losing too much style fidelity. On the other hand, CLIP score may not be perfectly aligned with the human intention  and would not capture the subtle style property.

Human Feedback (HF) is a more direct way of injecting user intention into the quality evaluation of synthetic images. HF is shown to be powerful and effective in LLM fine-tuning with reinforcement learning . In our case, HF could be used to compensate the CLIP score not being able to capture subtle style properties. Empirically, selecting less than a dozen images is enough for IT, and it only takes about 3 minutes per style. As shown in Section 4.4.4 and Figure 9, HF is critical for some applications, such as illustration designs, where capturing subtle differences is important to correctly reflect the designer’s intention. Nevertheless, due to human selection bias, style may drift or be reduced.

3.4 Sampling from Two theta's.

There has been an extensive study on personalization of text-to-image diffusion models to synthesize images containing multiple personal assets . In this section, we show how to combine DreamBooth and StyleDrop in a simple manner, thereby enabling personalization of both style and content. This is done by sampling from two modified generation distributions, guided by theta s for style and theta c for content, each of which are adapter parameters trained independently on style and content reference images, respectively. Unlike existing works , our approach does not require joint training of learnable parameters on multiple concepts, leading to a greater compositional power with pre-trained adapters, which are separately trained on individual subject and style assets.

Our overall sampling procedure follows the iterative decoding of Equation 1, with differences in how we sample logits at each decoding step. Let t be the text prompt and c be the text prompt without the style descriptor. ([3] For example, t is “A teapot in watercolor painting style” and c is “A teapot”.) We compute logits at step k as follows: lk=(1−gamma )lsk+gamma lck, where.

where gamma balances the StyleDrop and DreamBooth – if gamma is 0, we get StyleDrop, and DreamBooth if 1. By properly setting gamma (for example, 0.5∼0.7), we get images of my content in my style (see Figure 5).

4. Experiments.

We report results of StyleDrop on a variety of styles and compare with existing methods in Section 4.2. In Section 4.3 we show results on “my object in my style” combining the capability of DreamBooth and StyleDrop. Finally, we conduct an ablation study on the design choices of StyleDrop in Section 4.4. 4.1 Experimental Setting.

To the best of our knowledge, there has not been an extensive study of style-tuning for text-to-image generation models. As such, we suggest a new experimental protocol.

Data collection. We collect a few dozen images of various styles, from watercolor and oil painting, flat illustrations, 3d rendering to sculptures with varying materials. While painting styles have been a major focus for neural style transfer research , we go beyond and include a more diverse set of visual styles in our experiments. We provide image sources in Table S1 and attribute their ownership.

Model configuration. As in Section 3.2, we base StyleDrop on Muse  using adapter tuning . For all experiments, we update adapter weights for 1000 steps using Adam optimizer  with a learning rate of 0.00003. Unless otherwise stated, we use “StyleDrop” to denote the second round model trained on as many as 10 synthetic images with human feedback, as in Section 3.3. Nevertheless, to mitigate confusion, we append “HF” (human feedback), “CF” (CLIP feedback) or “R1” (first round model) to StyleDrop whenever there needs a clarity. More training details are in Section B.1. Evaluation. We report quantitative metrics based on CLIP  that measures the style consistency and textual alignment. In addition, we conduct the user preference study to assess style consistency and textual alignment. Section B.2 summarizes details on the human evaluation protocol.

4.2 StyleDrop Results.

Figure 1 shows results of our default approach on the 18 different style images that we collected, for the same text prompt. We see that StyleDrop is able to capture nuances of texture, shading, and structure across a wide range of styles, significantly better than previous approaches, enabling significantly more control over style than previously possible. Figure 4 shows synthesized images of StyleDrop using 3 different style reference images. For comparison, we also present results of (b) DreamBooth  on Imagen , (c) a LoRA implementation of DreamBooth  and (d) textual inversion , both on Stable Diffusion . ([4] Colab implementation of textual inversion  is used with stable-diffusion-2.), ([5] More baselines using hard prompt made easy (PEZ)  are in Appendix B.) More results are available in Figs. S12, S11, S10, S9, S13 and S8. For baselines, we follow instructions from the respective papers and open-source implementations, but with a few modifications. For example, instead of using a rare token (for example, “a watermelon slice in [V*] style”), we use the style descriptor (for example, “a watermelon slice in 3d rendering style”), similarly to StyleDrop. We train DreamBooth on Imagen for 300 steps after performing grid-search. This is less than 1000 steps recommended in , but is chosen to alleviate overfitting to image content and to better capture style. For LoRA DreamBooth on Stable Diffusion, we train for 400 steps with learning rates of 0.0002 for UNet and 0.000005 for CLIP. We do not adopt the iterative training for baselines in Figure 4. StyleDrop results without iterative training are in Section 4.4.3. It is clear from Figure 4 that StyleDrop on Muse convincingly outperforms other methods that are geared towards solving subject-driven personalization of text-to-image synthesis using diffusion models.

We see that style-tuning on Stable Diffusion with LoRA DreamBooth (Figure 4(c)) and textual inversion (Figure 4(d)) show poor style consistency to reference images. While DreamBooth on Imagen (Figure 4(b)) improves over those on Stable Diffusion, it still lacks the style consistency over StyleDrop on Muse across text prompts and style references. It is interesting to see such a difference as both Muse  and Imagen  are trained on the same set of image/text pairs using the same text encoder (T5-XXL ). We provide an ablation study to understand where the difference comes from in Section 4.4.1. 4.2.1 Quantitative Results.

For quantitative evaluation, we synthesize images from a subset of Parti prompts . This includes 190 text prompts of basic text compositions, while removing some categories such as abstract, arts, people or world knowledge. We test on 6 style reference images from Figure 1. ([6] Images used are (1, 2), (1, 6), (2, 3), (3, 1), (3, 5), (3, 6) of Figure 1, and are also visualized in Figure S7.)

CLIP scores. We employ two metrics using CLIP , (Text) and (Style) scores. For Text score, we measure the cosine similarity between image and text embeddings. For Style score, we measure the cosine similarity between embeddings of style reference and synthesized images. We generate 8 images per prompt for 190 text prompts, 1520 images in total. While we desire high scores, these metrics are not perfect. For example, Style score can easily get to 1.0 if mode collapses.

StyleDrop results in competitive Text scores to Muse (for example, 0.323 vs 0.322 of StyleDrop (HF)) while achieving significantly higher Style scores (for example, 0.556 vs 0.694 of StyleDrop (HF)), implying that synthesized images by StyleDrop are consistent in style with style reference images, without losing text-to-image generation capability. For the 6 styles we test, we see a light mode collapse from the first round of StyleDrop, resulting in a slightly reduced Text score. Iterative training (IT) improves the Text score, which is aligned with our motivation. As a trade-off, however, they show reduced Style scores over Round 1 models, as they are trained on synthetic images and styles may have been drifted due to a selection bias.

DreamBooth on Imagen falls short of StyleDrop in Style score (0.644 vs 0.694 of HF). We note that the increment in Style score for DreamBooth on Imagen is less significant (0.569 -> 0.644) than StyleDrop on Muse (0.556 -> 0.694). We think that the fine-tuning for style on Muse is more effective than that on Imagen. We revisit this in Section 4.4.1. Human evaluation. We formulate 3 binary comparison tasks for user preference among StyleDrop (R1), StyleDrop with different feedback signals, and DreamBooth on Imagen. Users are asked to select their preferred result in terms of style and text fidelity between images generated from two different models (that is, an A/B test), while given a style reference image and the text prompt. Details on the study is in Section B.2. Results are in Table 2 (top). Compared to DreamBooth on Imagen, images by StyleDrop are significantly more preferred by users in Style score. The user study also shows style drifting more clearly when comparing StyleDrop (R1) and StyleDrop IT either by HF or CF. Between HF and CF, HF retains better Style and CLIP retained better Text. Overall, we find that CLIP scores are a good proxy to the user study.

4.3 My Object in My Style.

We show in Figure 5 synthesized images by sampling from two personalized generation distributions, one for an object and another for the style, as described in Section 3.4. To learn object adapters, we use 5∼6 images per object. ([7] teapot and vase images from DreamBooth  are used.) Style adapters from Section 4.2 are used without any modification. The value of gamma (to balance the contribution of object and style adapters) is chosen in the range 0.5--0.7. We show synthesized images from (a) object adapter only (that is, DreamBooth), (b) style adapter only (that is, StyleDrop), and (c) both object and style adapters. We see from Figure 5(a) that text prompts are not sufficient to generate images with styles we desire. From Figure 5(b), though StyleDrop gets style correct, it generates objects that are inconsistent with reference subjects. The proposed sampling method from two distributions successfully captures both my object and my style, as in Figure 5(c).

4.4 Ablations.

We conduct ablations to better understand StyleDrop. In Section 4.4.1 we compare the behavior of the Imagen and Muse models. In Section 4.4.2 we highlight the importance of a style descriptor. In Section 4.4.3, we compare choices of feedback signals for iterative training. In Section 4.4.4, we show to what extent StyleDrop learns distinctive styles properties from reference images.

4.4.1 Comparative Study of DreamBooth on Imagen and StyleDrop on Muse.

We see in Section 4.2 that StyleDrop on Muse convincingly outperforms DreamBooth on Imagen. To better understand where the difference comes from, we conduct some control experiments.

Impact of training text prompt. We note that both experiments in Section 4.2 are carried out using the proposed descriptive style descriptors. To understand the contribution of fine-tuning, rather than the prompt engineering, we conduct control experiments, one with a rare token as in  (that is, “A flower in [V*] style”) and another with descriptive style prompt.

Results on Muse are in Figure 7. Comparing (a) and (c), we do not find a substantial change, suggesting that the style can be learned via an adapter tuning without too much help of a text prior. On the other hand, as seen in Figure 6, comparing (a) and (c) with Imagen as a backbone model, we see a notable difference. For example, “melting” property only appears for some images synthesized from a model trained with the descriptive style descriptor. This suggests that the learning capability of fine-tuning on Imagen may not be as powerful as that of Muse, when given only a few training images.

Data efficiency. Next, we study whether the quality of the fine-tuning on Imagen could be improved with more training data. In this study, we train a DreamBooth on Imagen using 10 human selected, synthetic images from StyleDrop on Muse. Results are in Figure 6. Two models are trained with (b) a rare token and (d) a descriptive style descriptor. We see that the style consistency improves a lot when comparing (c) and (d) of Figure 6, both in terms melting and golden properties. However, when using a rare token, we do not see any notable improvement from (a) to (b). This suggests that the superiority of StyleDrop on Muse may be coming from its extraordinary fine-tuning data efficiency.

4.4.2 Style Property Edit with Concept Disentanglement.

We show in Section 4.4.1 that StyleDrop on Muse is able to learn the style using a rare token identifier. Then, what is the benefit of descriptive style descriptor? We argue that not all styles are described in a single word and the user may want to learn style properties selectively. For example, the style of an image in Figure 7 may be written as a composite of “melting”, “golden”, and “3d rendering”, but the user may want to learn its “golden 3d rendering” style without “melting”.

We show that such a style property edit can be naturally done with a descriptive style descriptor. As in Figure 7(d), learning with a descriptive style descriptor provides an extra knob to edit a style by omitting certain words (for example, “melting”) from the style descriptor at synthesis. This clearly shows the benefit of descriptive style descriptors in disentangling visual concepts and creating a new style based on an existing one. This is less amenable when trained with the rare token, as in Figure 7(b).

4.4.3 Iterative Training with Different Feedback Signals.

We study how different feedback signals affects the performance of StyleDrop. We compare three feedback signals, including human, CLIP, and random. For CLIP and random signals, we synthesize 64 images per prompt from 30 text prompts and select one image per prompt. For human, we select 10 images from the same pool, which takes about 3 minutes per style. See Section B.2 for details.

Qualitative results are in Figure 8. We observe that some images in (a) from a Round 1 model show a mix of banana or bottle with a human. Such concept leakage is alleviated with IT, though we still see a banana with arms and legs with Random strategy. The reduction in concept leakage could be verified with the Text score, achieving (a) 0.303, (b) 0.322, (c) 0.339, and (d) 0.328. On the other hand, Style score, (a) 0.560, (b) 0.567, (c) 0.542, and (d) 0.549, could be misleading in this case, as we compute the visual similarity to the style reference image, favoring a content leakage. Between CLIP and human feedback, we see a clear trade-off between text and style fidelity from quantitative metrics.

4.4.4 Fine-Grained Style Control with User Intention.

Moreover, human feedback is more critical when trying to capture subtle style properties. In this study, we conduct experiments on four images in Figure 9 inside orange boxes, created by the same designer with varying style properties, such as color offset (Figure 9(b)), gradation (Figure 9(c)), and sharp corners (Figure 9(d)). We train two more rounds of StyleDrop with human feedback. We use the same style descriptor of “minimal flat illustration style” to make sure the same text prior is given to all experiments. As in Figure 9, style properties such as color offset, gradation, and corner shape are captured correctly. This suggests that StyleDrop offers the control of fine-grained style variations.

5. Conclusion.

We have presented StyleDrop, a novel approach to enable synthesis of any style through the use of a few user-provided images of that style and a text description. Built on Muse  using adapter tuning , StyleDrop achieves remarkable style consistency at text-to-image synthesis. Training StyleDrop is efficient both in the number of learnable parameters (for example, &lt; 1%) and the number of style samples (for example, 1) required.

Limitations. Visual styles are of course even more diverse than what is possible to explore in our paper. More study with a well-defined system of visual styles, including, but not limited to, the formal attributes (for example, use of color, composition, shading), media (for example, line drawing, etching, oil painting), history and era (for example, Renaissance painting, medieval mosaics, Art Deco), and style of art (for example, Cubism, Minimalism, Pop Art), would broaden the scope. While we show in part the superiority of a generative vision transformer to diffusion models at few-shot transfer learning, it is by no means conclusive. We leave an in-depth study among text-to-image generation models as a future work.

Societal impact. As illustrated in Figure 4, StyleDrop could be used to improve the productivity and creativity of art directors and graphic designers when generating various visual assets in their own style. StyleDrop makes it easy to reproduce many personalized visual assets from as little as one seed image. We recognize potential pitfalls such as the ability to copy individual artists’ styles without their consent, and urge the responsible use of our technology.

Acknowledgement. We thank Varun Jampani, Jason Baldridge, Forrester Cole, José Lezama, Steven Hickson, Kfir Aberman for their valuable feedback on our manuscript.
