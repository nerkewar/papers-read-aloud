<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:podcast="https://podcastindex.org/namespace/1.0"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>Papers Read Aloud</title>
    <link>https://nerkewar.github.io/papers-read-aloud/</link>
    <language>en</language>
    <pubDate>Wed, 23 Aug 2023 03:33:38 +0000</pubDate>
    <lastBuildDate>Wed, 23 Aug 2023 03:33:38 +0000</lastBuildDate>
    <copyright>Copyright © 2023 nerkewar</copyright>
    <itunes:author>nerkewar</itunes:author>
    <description>
      Papers Read Aloud is a podcast featuring scientific papers read by a lovely robot.
    </description>
    <itunes:image
      href="https://nerkewar.github.io/papers-read-aloud/image.png" />
      <image>
        <url>
          https://nerkewar.github.io/papers-read-aloud
        </url>
        <title>Papers Read Aloud</title>
        <link>https://nerkewar.github.io/papers-read-aloud</link>
      </image>
    <itunes:category text="Science"></itunes:category>
    <itunes:owner>
      <itunes:name>nerkewar</itunes:name>
      <itunes:email>sables-probe-0i@icloud.com</itunes:email>
    </itunes:owner>
    <itunes:type>episodic</itunes:type>
    <itunes:keywords />
    <itunes:complete>No</itunes:complete>
    <itunes:explicit>No</itunes:explicit>
    <podcast:locked owner="sables-probe-0i@icloud.com">no</podcast:locked>
    <podcast:medium>podcast</podcast:medium>
    <item>
        <title>Abstract</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2307.05977</guid>
        <description>
        <![CDATA[<p>Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called sdd to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time. Code is available at https://github.com/nannullna/safe-diffusion.</p>]]>
        </description>
        <pubDate>Sun, 10 Sep 2023 20:03:24 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2307.05977/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>7</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>7</podcast:episode>
    </item>
    <item>
        <title>InstructPix2Pix: Learning to Follow Image Editing Instructions</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2211.09800</guid>
        <description>
        <![CDATA[<p>We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 05:09:00 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2211.09800/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>6</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>6</podcast:episode>
    </item>
    <item>
        <title>GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2302.04222</guid>
        <description>
        <![CDATA[<p>Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after “fine-tuning” on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply “style cloaks” to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05),<br>Glaze is highly successful at disrupting mimicry under normal conditions (&gt;92%) and against adaptive countermeasures (&gt;85%).</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:44:08 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2302.04222/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>5</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>5</podcast:episode>
    </item>
    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p>Evaluation of generative models has been an underrepresented field despite the surge of generative architectures. Most recent models are evaluated upon rather obsolete metrics which suffer from robustness issues, while being unable to assess more aspects of visual quality, such as compositionality and logic of synthesis. At the same time, the explainability of generative models remains a limited, though important, research direction with several current attempts requiring access to the inner functionalities of generative models. Contrary to prior literature, we view generative models as a black box, and we propose a framework for the evaluation and explanation of synthesized results based on concepts instead of pixels. Our framework exploits knowledge-based counterfactual edits that underline which objects or attributes should be inserted, removed, or replaced from generated images to approach their ground truth conditioning. Moreover, global explanations produced by accumulating local edits can also reveal what concepts a model cannot generate in total. The application of our framework on various models designed for the challenging tasks of Story Visualization and Scene Synthesis verifies the power of our approach in the model-agnostic setting.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:39:02 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>4</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>4</podcast:episode>
    </item>
    <item>
      <title>LayerDiffusion: Layered Controlled Image Editing with Diffusion Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:40:05 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>3</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>3</podcast:episode>
    </item>
    <item>
      <title>StyleDrop: Text-to-Image Generation in Any Style</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:33:30 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>2</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>2</podcast:episode>
    </item>
    <item>
      <title>RISE: Randomized Input Sampling for Explanation of Black-box Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421</guid>
      <description>
        <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:05:30 +0000</pubDate>
      <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421/audio.mp3"
        length="10508164" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>1</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>1</podcast:episode>
    </item>
  </channel>
</rss>