<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:podcast="https://podcastindex.org/namespace/1.0"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>Papers Read Aloud</title>
    <link>https://nerkewar.github.io/papers-read-aloud/</link>
    <language>en</language>
    <pubDate>Wed, 23 Aug 2023 03:33:38 +0000</pubDate>
    <lastBuildDate>Wed, 23 Aug 2023 03:33:38 +0000</lastBuildDate>
    <copyright>Copyright Â© 2023 nerkewar</copyright>
    <itunes:author>nerkewar</itunes:author>
    <description>
      Papers Read Aloud is a podcast featuring scientific papers read by a lovely robot.
    </description>
    <itunes:image
      href="https://nerkewar.github.io/papers-read-aloud/image.png" />
      <image>
        <url>
          https://nerkewar.github.io/papers-read-aloud
        </url>
        <title>Papers Read Aloud</title>
        <link>https://nerkewar.github.io/papers-read-aloud</link>
      </image>
    <itunes:category text="Science"></itunes:category>
    <itunes:owner>
      <itunes:name>nerkewar</itunes:name>
      <itunes:email>sables-probe-0i@icloud.com</itunes:email>
    </itunes:owner>
    <itunes:type>episodic</itunes:type>
    <itunes:keywords />
    <itunes:complete>No</itunes:complete>
    <itunes:explicit>No</itunes:explicit>
    <podcast:locked owner="sables-probe-0i@icloud.com">no</podcast:locked>
    <podcast:medium>podcast</podcast:medium>

    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p>Evaluation of generative models has been an underrepresented field despite the surge of generative architectures. Most recent models are evaluated upon rather obsolete metrics which suffer from robustness issues, while being unable to assess more aspects of visual quality, such as compositionality and logic of synthesis. At the same time, the explainability of generative models remains a limited, though important, research direction with several current attempts requiring access to the inner functionalities of generative models. Contrary to prior literature, we view generative models as a black box, and we propose a framework for the evaluation and explanation of synthesized results based on concepts instead of pixels. Our framework exploits knowledge-based counterfactual edits that underline which objects or attributes should be inserted, removed, or replaced from generated images to approach their ground truth conditioning. Moreover, global explanations produced by accumulating local edits can also reveal what concepts a model cannot generate in total. The application of our framework on various models designed for the challenging tasks of Story Visualization and Scene Synthesis verifies the power of our approach in the model-agnostic setting.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:39:02 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>7</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>7</podcast:episode>
    </item>

    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p>Evaluation of generative models has been an underrepresented field despite the surge of generative architectures. Most recent models are evaluated upon rather obsolete metrics which suffer from robustness issues, while being unable to assess more aspects of visual quality, such as compositionality and logic of synthesis. At the same time, the explainability of generative models remains a limited, though important, research direction with several current attempts requiring access to the inner functionalities of generative models. Contrary to prior literature, we view generative models as a black box, and we propose a framework for the evaluation and explanation of synthesized results based on concepts instead of pixels. Our framework exploits knowledge-based counterfactual edits that underline which objects or attributes should be inserted, removed, or replaced from generated images to approach their ground truth conditioning. Moreover, global explanations produced by accumulating local edits can also reveal what concepts a model cannot generate in total. The application of our framework on various models designed for the challenging tasks of Story Visualization and Scene Synthesis verifies the power of our approach in the model-agnostic setting.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:35:43 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>6</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>6</podcast:episode>
    </item>

    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p><br></p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:34:10 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>5</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>5</podcast:episode>
    </item>
    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p><br></p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 03:58:09 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>4</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>4</podcast:episode>
    </item>
    <item>
      <title>LayerDiffusion: Layered Controlled Image Editing with Diffusion Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:40:05 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>3</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>3</podcast:episode>
    </item>
    <item>
      <title>StyleDrop: Text-to-Image Generation in Any Style</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:33:30 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>2</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>2</podcast:episode>
    </item>
    <item>
      <title>RISE: Randomized Input Sampling for Explanation of Black-box Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421</guid>
      <description>
        <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:05:30 +0000</pubDate>
      <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421/audio.mp3"
        length="10508164" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>1</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>1</podcast:episode>
    </item>
  </channel>
</rss>