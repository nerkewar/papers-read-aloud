<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:podcast="https://podcastindex.org/namespace/1.0"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>Papers Read Aloud</title>
    <link>https://nerkewar.github.io/papers-read-aloud/</link>
    <language>en</language>
    <pubDate>Wed, 23 Aug 2023 03:33:38 +0000</pubDate>
    <lastBuildDate>Wed, 23 Aug 2023 03:33:38 +0000</lastBuildDate>
    <copyright>Copyright © 2023 nerkewar</copyright>
    <itunes:author>nerkewar</itunes:author>
    <description>
      Papers Read Aloud is a podcast featuring scientific papers read by a lovely robot.
    </description>
    <itunes:image
      href="https://nerkewar.github.io/papers-read-aloud/image.png" />
      <image>
        <url>
          https://nerkewar.github.io/papers-read-aloud
        </url>
        <title>Papers Read Aloud</title>
        <link>https://nerkewar.github.io/papers-read-aloud</link>
      </image>
    <itunes:category text="Science"></itunes:category>
    <itunes:owner>
      <itunes:name>nerkewar</itunes:name>
      <itunes:email>sables-probe-0i@icloud.com</itunes:email>
    </itunes:owner>
    <itunes:type>episodic</itunes:type>
    <itunes:keywords />
    <itunes:complete>No</itunes:complete>
    <itunes:explicit>No</itunes:explicit>
    <podcast:locked owner="sables-probe-0i@icloud.com">no</podcast:locked>
    <podcast:medium>podcast</podcast:medium>
    <item>
        <title>Multi-Concept Customization of Text-to-Image Diffusion</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2212.04488</guid>
        <description>
        <![CDATA[<p>While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (\vbox∼6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple, new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms several baselines and concurrent works, regarding both qualitative and quantitative evaluations, while being memory and computationally efficient.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:48:34 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2212.04488/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>21</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>21</podcast:episode>
    </item>
    <item>
        <title>ReVersion: Diffusion-Based Relation Inversion from Images</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.13495</guid>
        <description>
        <![CDATA[<p>Diffusion models gain increasing popularity for their generative capabilities. Recently, there have been surging needs to generate customized images by inverting diffusion models from exemplar images. However, existing inversion methods mainly focus on capturing object appearances. How to invert object relations, another important pillar in the visual world, remains unexplored. In this work, we propose ReVersion for the Relation Inversion task, which aims to learn a specific relation (represented as “relation prompt”) from exemplar images. Specifically, we learn a relation prompt from a frozen pre-trained text-to-image diffusion model. The learned relation prompt can then be applied to generate relation-specific images with new objects, backgrounds, and styles.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:48:26 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.13495/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>20</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>20</podcast:episode>
    </item>
    <item>
        <title>Designing an Encoder for Fast Personalization of Text-to-Image Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2302.12228</guid>
        <description>
        <![CDATA[<p>Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based domain-tuning approach. Our key insight is that by underfitting on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, for example a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively ingest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps — accelerating personalization from dozens of minutes to seconds, while preserving quality.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:47:29 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2302.12228/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>19</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>19</podcast:episode>
    </item>
    <item>
        <title>Data Redaction from Conditional Generative Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.11351</guid>
        <description>
        <![CDATA[<p>Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:44:53 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.11351/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>18</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>18</podcast:episode>
    </item>
    <item>
        <title>Understanding and Mitigating Copying</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.20086</guid>
        <description>
        <![CDATA[<p>Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set. Code is available at https://github.com/somepago/DCR.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:44:21 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.20086/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>17</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>17</podcast:episode>
    </item>
    <item>
        <title>Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.10120</guid>
        <description>
        <![CDATA[<p>The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:43:50 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.10120/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>16</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>16</podcast:episode>
    </item>
    <item>
        <title>Parts of Speech–Grounded⁠ Subspaces</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.14053</guid>
        <description>
        <![CDATA[<p>Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased towards specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP’s joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (for example nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What’s more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (for example artists’ painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists’ styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification. Our code is available at: https://github.com/james-oldfield/PoS-subspaces.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:43:21 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.14053/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>15</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>15</podcast:episode>
    </item>
    <item>
        <title>Reference-based Image Composition with Sketch</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2304.09748</guid>
        <description>
        <![CDATA[<p>Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (that is, sketch) and content (that is, reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:41:30 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2304.09748/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>14</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>14</podcast:episode>
    </item>
    <item>
        <title>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.10431</guid>
        <description>
        <![CDATA[<p>Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend features among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300×-2500× speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:41:07 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.10431/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>13</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>13</podcast:episode>
    </item>
    <item>
        <title>Key-Locked Rank One Editing for Text-to-Image Personalization</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.01644</guid>
        <description>
        <![CDATA[<p>Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:40:39 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.01644/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>12</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>12</podcast:episode>
    </item>
    <item>
        <title>What the DAAM: Interpreting Stable Diffusion Using Cross Attention</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2210.04885</guid>
        <description>
        <![CDATA[<p>Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, with some performing similar to real photographs in human evaluation. However, they remain poorly understood, lacking explainability and interpretability analyses, largely due to their proprietary, closed-source nature. In this paper, to shine some much-needed light on text-to-image diffusion models, we perform a text–image attribution analysis on Stable Diffusion, a recently open-sourced large diffusion model. To produce pixel-level attribution maps, we propose DAAM, a novel method based on upscaling and aggregating cross-attention activations in the latent denoising subnetwork. We support its correctness by evaluating its unsupervised semantic segmentation quality on its own generated imagery, compared to supervised segmentation models. We show that DAAM performs strongly on COCO caption-generated images, achieving an mIoU of 61.0, and it outperforms supervised models on open-vocabulary segmentation, for an mIoU of 51.5. We further find that certain parts of speech, like punctuation and conjunctions, influence the generated imagery most, which agrees with the prior literature, while determiners and numerals the least, suggesting poor numeracy. To our knowledge, we are the first to propose and study word–pixel attribution for large-scale text-to-image diffusion models. Our code and data are at https://github.com/castorini/daam.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:39:49 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2210.04885/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>11</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>11</podcast:episode>
    </item>
    <item>
        <title>Ensuring Visual Commonsense Morality for Text-to-Image Generation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2212.03507</guid>
        <description>
        <![CDATA[<p>Text-to-image generation methods produce high-resolution and high-quality images, but these methods should not produce immoral images that may contain inappropriate content from the perspective of commonsense morality. In this paper, we aim to automatically judge the immorality of synthesized images and manipulate these images into morally acceptable alternatives. To this end, we build a model that has three main primitives: (1) recognition of the visual commonsense immorality in a given image, (2) localization or highlighting of immoral visual (and textual) attributes that contribute to the immorality of the image, and (3) manipulation of an immoral image to create a morally-qualifying alternative. We conduct experiments and human studies using the state-of-the-art Stable Diffusion text-to-image generation model, demonstrating the effectiveness of our ethical image manipulation approach.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:38:58 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2212.03507/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>10</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>10</podcast:episode>
    </item>
    <item>
        <title>StyleDrop: Text-to-Image Generation in Any Style</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983</guid>
        <description>
        <![CDATA[<p>Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce StyleDrop, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than 1% of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a single image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse  convincingly outperforms other methods, including DreamBooth  and textual inversion  on Imagen  or Stable Diffusion . More results are available at our project website: https://styledrop.github.io.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:38:23 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>9</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>9</podcast:episode>
    </item>
    <item>
        <title>LayerDiffusion: Layered Controlled Image Editing with Diffusion Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676</guid>
        <description>
        <![CDATA[<p>Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods.<br>LayerDiffusion opens up new possibilities for controllable image editing.</p>]]>
        </description>
        <pubDate>Mon, 11 Sep 2023 21:37:19 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>8</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>8</podcast:episode>
    </item>
    <item>
        <title>Abstract</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2307.05977</guid>
        <description>
        <![CDATA[<p>Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called sdd to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time. Code is available at https://github.com/nannullna/safe-diffusion.</p>]]>
        </description>
        <pubDate>Sun, 10 Sep 2023 20:03:24 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2307.05977/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>7</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>7</podcast:episode>
    </item>
    <item>
        <title>InstructPix2Pix: Learning to Follow Image Editing Instructions</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2211.09800</guid>
        <description>
        <![CDATA[<p>We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 05:09:00 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2211.09800/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>6</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>6</podcast:episode>
    </item>
    <item>
        <title>GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2302.04222</guid>
        <description>
        <![CDATA[<p>Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after “fine-tuning” on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply “style cloaks” to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05),<br>Glaze is highly successful at disrupting mimicry under normal conditions (&gt;92%) and against adaptive countermeasures (&gt;85%).</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:44:08 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2302.04222/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>5</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>5</podcast:episode>
    </item>
    <item>
        <title>Counterfactual Edits for Generative Evaluation</title>
        <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555</guid>
        <description>
        <![CDATA[<p>Evaluation of generative models has been an underrepresented field despite the surge of generative architectures. Most recent models are evaluated upon rather obsolete metrics which suffer from robustness issues, while being unable to assess more aspects of visual quality, such as compositionality and logic of synthesis. At the same time, the explainability of generative models remains a limited, though important, research direction with several current attempts requiring access to the inner functionalities of generative models. Contrary to prior literature, we view generative models as a black box, and we propose a framework for the evaluation and explanation of synthesized results based on concepts instead of pixels. Our framework exploits knowledge-based counterfactual edits that underline which objects or attributes should be inserted, removed, or replaced from generated images to approach their ground truth conditioning. Moreover, global explanations produced by accumulating local edits can also reveal what concepts a model cannot generate in total. The application of our framework on various models designed for the challenging tasks of Story Visualization and Scene Synthesis verifies the power of our approach in the model-agnostic setting.</p>]]>
        </description>
        <pubDate>Wed, 23 Aug 2023 04:39:02 +0000</pubDate>
        <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/2303.01555/audio.mp3" type="audio/mpeg" />
        <itunes:author>nerkewar</itunes:author>
        <itunes:explicit>No</itunes:explicit>
        <itunes:episodeType>Full</itunes:episodeType>
        <itunes:episode>4</itunes:episode>
        <itunes:keywords></itunes:keywords>
        <itunes:season>1</itunes:season>
        <podcast:episode>4</podcast:episode>
    </item>
    <item>
      <title>LayerDiffusion: Layered Controlled Image Editing with Diffusion Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:40:05 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2305.18676/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>3</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>3</podcast:episode>
    </item>
    <item>
      <title>StyleDrop: Text-to-Image Generation in Any Style</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983</guid>
      <description>
      <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:33:30 +0000</pubDate>
      <enclosure
      url="https://nerkewar.github.io/papers-read-aloud/episodes/2306.00983/audio.mp3" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>2</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>2</podcast:episode>
    </item>
    <item>
      <title>RISE: Randomized Input Sampling for Explanation of Black-box Models</title>
      <guid isPermaLink="false">https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421</guid>
      <description>
        <![CDATA[<p><br></p>]]>
      </description>
      <pubDate>Wed, 23 Aug 2023 03:05:30 +0000</pubDate>
      <enclosure
        url="https://nerkewar.github.io/papers-read-aloud/episodes/1806.07421/audio.mp3"
        length="10508164" type="audio/mpeg" />
      <itunes:author>nerkewar</itunes:author>
      <itunes:explicit>No</itunes:explicit>
      <itunes:episodeType>Full</itunes:episodeType>
      <itunes:episode>1</itunes:episode>
      <itunes:keywords></itunes:keywords>
      <itunes:season>1</itunes:season>
      <podcast:episode>1</podcast:episode>
    </item>
  </channel>
</rss>